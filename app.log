2024-12-25 11:40:25,462 - __main__ - INFO - Successfully loaded QA model
2024-12-25 11:40:25,470 - __main__ - INFO - Starting application...
2024-12-25 11:40:25,470 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 11:40:25,470 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 11:40:25,470 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 11:40:25,510 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 11:40:25,510 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 11:40:25,521 - werkzeug - INFO -  * Restarting with stat
2024-12-25 11:40:38,741 - __main__ - INFO - Successfully loaded QA model
2024-12-25 11:40:38,749 - __main__ - INFO - Starting application...
2024-12-25 11:40:38,749 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 11:40:38,767 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 11:40:38,769 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 11:40:38,814 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 11:40:38,823 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 11:40:38,910 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:40:38] "GET / HTTP/1.1" 200 -
2024-12-25 11:40:39,066 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:40:39] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 11:40:39,407 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:40:39] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 11:41:04,513 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:04] "GET / HTTP/1.1" 200 -
2024-12-25 11:41:04,564 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:04] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 11:41:04,580 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:04] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 11:41:14,756 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /upload
2024-12-25 11:41:14,756 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:14] "[31m[1mPOST /upload HTTP/1.1[0m" 403 -
2024-12-25 11:41:43,923 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /upload
2024-12-25 11:41:43,928 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:43] "[31m[1mPOST /upload HTTP/1.1[0m" 403 -
2024-12-25 11:41:47,894 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:47] "GET / HTTP/1.1" 200 -
2024-12-25 11:41:47,943 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:47] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 11:41:47,968 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:47] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 11:41:53,935 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /upload
2024-12-25 11:41:53,943 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:41:53] "[31m[1mPOST /upload HTTP/1.1[0m" 403 -
2024-12-25 11:43:26,049 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 11:43:27,320 - werkzeug - INFO -  * Restarting with stat
2024-12-25 11:43:41,413 - __main__ - INFO - Successfully loaded QA model
2024-12-25 11:43:41,413 - __main__ - INFO - Starting application...
2024-12-25 11:43:41,413 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 11:43:41,413 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 11:43:41,413 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 11:43:41,470 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 11:43:41,478 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 11:47:32,641 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 11:47:34,115 - werkzeug - INFO -  * Restarting with stat
2024-12-25 11:49:14,684 - __main__ - INFO - Successfully loaded QA model
2024-12-25 11:49:14,684 - __main__ - INFO - Starting application...
2024-12-25 11:49:14,684 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 11:49:14,692 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 11:49:14,692 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 11:49:14,741 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 11:49:14,741 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 11:49:14,749 - werkzeug - INFO -  * Restarting with stat
2024-12-25 11:49:29,807 - __main__ - INFO - Successfully loaded QA model
2024-12-25 11:49:29,815 - __main__ - INFO - Starting application...
2024-12-25 11:49:29,815 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 11:49:29,841 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 11:49:29,841 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 11:49:29,876 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 11:49:29,889 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 11:49:30,028 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:49:30] "GET / HTTP/1.1" 200 -
2024-12-25 11:49:30,309 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:49:30] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 11:49:30,317 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:49:30] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 11:50:08,529 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:50:08] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 11:51:17,704 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 11:51:19,035 - werkzeug - INFO -  * Restarting with stat
2024-12-25 11:51:31,075 - __main__ - INFO - Successfully loaded QA model
2024-12-25 11:51:31,078 - __main__ - INFO - Starting application...
2024-12-25 11:51:31,078 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 11:51:31,078 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 11:51:31,078 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 11:51:31,087 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 11:51:31,094 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 11:51:31,152 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:51:31] "GET / HTTP/1.1" 200 -
2024-12-25 11:51:31,314 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:51:31] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 11:51:31,314 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:51:31] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 11:51:44,336 - __main__ - INFO - File saved successfully: 9f2be57f421b7c83a789183258d2fe4451a3a28feb33cee6bfedbcdce25cc450.pdf
2024-12-25 11:51:45,133 - __main__ - INFO - Temporary file removed: 9f2be57f421b7c83a789183258d2fe4451a3a28feb33cee6bfedbcdce25cc450.pdf
2024-12-25 11:51:45,133 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:51:45] "POST /upload HTTP/1.1" 200 -
2024-12-25 11:52:01,140 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:52:01] "POST /ask HTTP/1.1" 200 -
2024-12-25 11:52:23,893 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:52:23] "POST /ask HTTP/1.1" 200 -
2024-12-25 11:52:40,255 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:52:40] "POST /ask HTTP/1.1" 200 -
2024-12-25 11:56:10,234 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:56:10] "GET / HTTP/1.1" 200 -
2024-12-25 11:56:10,282 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:56:10] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-25 11:56:10,282 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:56:10] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 11:56:22,280 - __main__ - INFO - File saved successfully: 9af2e75196204095b1934b4d495e7cb4f6c049ee0a15e1b547dbe7319204eebd.pdf
2024-12-25 11:56:23,052 - __main__ - INFO - Temporary file removed: 9af2e75196204095b1934b4d495e7cb4f6c049ee0a15e1b547dbe7319204eebd.pdf
2024-12-25 11:56:23,053 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:56:23] "POST /upload HTTP/1.1" 200 -
2024-12-25 11:56:32,577 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:56:32] "POST /ask HTTP/1.1" 200 -
2024-12-25 11:56:32,593 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:56:32] "POST /ask HTTP/1.1" 200 -
2024-12-25 11:56:49,887 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 11:56:49] "POST /ask HTTP/1.1" 200 -
2024-12-25 12:39:29,682 - __main__ - INFO - Successfully loaded QA model
2024-12-25 12:39:29,684 - __main__ - INFO - Starting application...
2024-12-25 12:39:29,685 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 12:39:29,685 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 12:39:29,685 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 12:39:29,709 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 12:39:29,710 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 12:39:29,712 - werkzeug - INFO -  * Restarting with stat
2024-12-25 12:39:35,315 - __main__ - INFO - Successfully loaded QA model
2024-12-25 12:39:35,316 - __main__ - INFO - Starting application...
2024-12-25 12:39:35,316 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 12:39:35,317 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 12:39:35,317 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 12:39:35,321 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 12:39:35,323 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 12:39:35,367 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:39:35] "GET / HTTP/1.1" 200 -
2024-12-25 12:39:35,524 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:39:35] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-25 12:39:35,524 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:39:35] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 12:39:47,914 - __main__ - INFO - File saved successfully: 0e5e57f1b8832d71b53ddf45c53bb58bd506d2293d6a432a0fed295ab38393c7.pdf
2024-12-25 12:39:48,720 - __main__ - INFO - Temporary file removed: 0e5e57f1b8832d71b53ddf45c53bb58bd506d2293d6a432a0fed295ab38393c7.pdf
2024-12-25 12:39:48,722 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:39:48] "POST /upload HTTP/1.1" 200 -
2024-12-25 12:40:10,700 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:40:10] "POST /ask HTTP/1.1" 200 -
2024-12-25 12:40:10,705 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:40:10] "POST /ask HTTP/1.1" 200 -
2024-12-25 12:45:17,608 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:45:17] "GET / HTTP/1.1" 200 -
2024-12-25 12:45:17,650 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:45:17] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-25 12:49:46,846 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:49:46] "GET / HTTP/1.1" 200 -
2024-12-25 12:49:46,862 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:49:46] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-25 12:49:46,873 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:49:46] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 12:50:01,628 - __main__ - INFO - File saved successfully: b70fe5dc461eeb82b5ed198c9900c3c58d43222525ce84ba602201e986c1292f.pdf
2024-12-25 12:50:02,940 - __main__ - INFO - Temporary file removed: b70fe5dc461eeb82b5ed198c9900c3c58d43222525ce84ba602201e986c1292f.pdf
2024-12-25 12:50:02,947 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:50:02] "POST /upload HTTP/1.1" 200 -
2024-12-25 12:50:23,334 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 12:50:23] "POST /ask HTTP/1.1" 200 -
2024-12-25 13:40:11,633 - __main__ - INFO - Successfully loaded QA model
2024-12-25 13:40:11,634 - __main__ - INFO - Starting application...
2024-12-25 13:40:11,634 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 13:40:11,635 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 13:40:11,635 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 13:40:11,649 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 13:40:11,649 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 13:40:11,651 - werkzeug - INFO -  * Restarting with stat
2024-12-25 13:40:19,076 - __main__ - INFO - Successfully loaded QA model
2024-12-25 13:40:19,081 - __main__ - INFO - Starting application...
2024-12-25 13:40:19,081 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 13:40:19,082 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 13:40:19,083 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 13:40:19,099 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 13:40:19,108 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 13:40:19,256 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:40:19] "GET / HTTP/1.1" 200 -
2024-12-25 13:40:19,573 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:40:19] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 13:40:19,585 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:40:19] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-25 13:40:32,196 - __main__ - INFO - File saved successfully: 1f48d4f1b8d7e3667c2948542099c82e037a459bc05fc6cc2b8105c83bc49846.pdf
2024-12-25 13:40:32,989 - __main__ - INFO - Temporary file removed: 1f48d4f1b8d7e3667c2948542099c82e037a459bc05fc6cc2b8105c83bc49846.pdf
2024-12-25 13:40:32,991 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:40:32] "POST /upload HTTP/1.1" 200 -
2024-12-25 13:41:08,071 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:41:08] "POST /ask HTTP/1.1" 200 -
2024-12-25 13:41:08,223 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:41:08] "POST /ask HTTP/1.1" 200 -
2024-12-25 13:46:42,535 - __main__ - INFO - File saved successfully: 49ee9d5933fc9b6b11090367b8b87b7751a9bb02ddc7351c19d3d8146a3505ce.pdf
2024-12-25 13:46:42,559 - __main__ - INFO - Temporary file removed: 49ee9d5933fc9b6b11090367b8b87b7751a9bb02ddc7351c19d3d8146a3505ce.pdf
2024-12-25 13:46:42,559 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:46:42] "POST /upload HTTP/1.1" 200 -
2024-12-25 13:46:53,270 - __main__ - WARNING - Hallucinations detected: {'Eiffel Tower'}
2024-12-25 13:46:53,271 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:46:53] "POST /ask HTTP/1.1" 200 -
2024-12-25 13:47:23,561 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 13:47:23] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:40:00,492 - __main__ - INFO - Successfully loaded QA model
2024-12-25 14:40:00,492 - __main__ - INFO - Starting application...
2024-12-25 14:40:00,492 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 14:40:00,492 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 14:40:00,492 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 14:40:00,515 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 14:40:00,515 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 14:40:00,525 - werkzeug - INFO -  * Restarting with stat
2024-12-25 14:40:14,325 - __main__ - INFO - Successfully loaded QA model
2024-12-25 14:40:14,333 - __main__ - INFO - Starting application...
2024-12-25 14:40:14,333 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 14:40:14,333 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 14:40:14,333 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 14:40:14,333 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 14:40:14,342 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 14:40:57,689 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:40:57] "GET / HTTP/1.1" 200 -
2024-12-25 14:40:57,828 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:40:57] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 14:40:57,828 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:40:57] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-25 14:41:05,344 - __main__ - INFO - File saved successfully: 1838d4f25f652a84cb1d1bbc0bb05388202931cae1eb59bcaf81fa39d9963db7.pdf
2024-12-25 14:41:05,368 - __main__ - INFO - Temporary file removed: 1838d4f25f652a84cb1d1bbc0bb05388202931cae1eb59bcaf81fa39d9963db7.pdf
2024-12-25 14:41:05,377 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:41:05] "POST /upload HTTP/1.1" 200 -
2024-12-25 14:41:15,587 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:41:15] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:41:29,415 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.15).
2024-12-25 14:41:29,455 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:41:29] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:41:47,595 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:41:47] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:46:19,361 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.08).
2024-12-25 14:46:19,394 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:46:19] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:48:01,364 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 14:48:03,368 - werkzeug - INFO -  * Restarting with stat
2024-12-25 14:48:25,207 - __main__ - INFO - Successfully loaded QA model
2024-12-25 14:48:25,212 - __main__ - INFO - Starting application...
2024-12-25 14:48:25,212 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 14:48:25,220 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 14:48:25,224 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 14:48:25,254 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 14:48:25,261 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 14:48:25,464 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:48:25] "GET / HTTP/1.1" 200 -
2024-12-25 14:48:25,871 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:48:25] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 14:48:25,903 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:48:25] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 14:48:33,030 - __main__ - INFO - File saved successfully: 0fe0a4afda26389f75f46acba2432f8e56ce6a0830c66ad483141c01debb90bc.pdf
2024-12-25 14:48:33,038 - __main__ - INFO - Temporary file removed: 0fe0a4afda26389f75f46acba2432f8e56ce6a0830c66ad483141c01debb90bc.pdf
2024-12-25 14:48:33,038 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:48:33] "POST /upload HTTP/1.1" 200 -
2024-12-25 14:48:40,651 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:48:40] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:48:40,651 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:48:40] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:48:53,624 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.07).
2024-12-25 14:48:53,632 - __main__ - WARNING - Hallucinated entities detected: {'one'}
2024-12-25 14:48:53,640 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:48:53] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:49:26,747 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 14:49:27,701 - werkzeug - INFO -  * Restarting with stat
2024-12-25 14:49:42,198 - __main__ - INFO - Successfully loaded QA model
2024-12-25 14:49:42,206 - __main__ - INFO - Starting application...
2024-12-25 14:49:42,206 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 14:49:42,206 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 14:49:42,214 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 14:49:42,254 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 14:49:42,264 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 14:49:42,402 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-25 14:49:42,402 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-25 14:49:42,410 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:49:42] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-25 14:49:42,418 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:49:42] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-25 14:49:53,769 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /upload
2024-12-25 14:49:53,776 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:49:53] "[31m[1mPOST /upload HTTP/1.1[0m" 403 -
2024-12-25 14:49:55,985 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:49:55] "GET / HTTP/1.1" 200 -
2024-12-25 14:49:56,260 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:49:56] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 14:49:56,303 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:49:56] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 14:50:02,596 - __main__ - INFO - File saved successfully: 4e2169daac2f80d3c75786635a391eb004e4f3f6bb4834f37f3902340fc12985.pdf
2024-12-25 14:50:02,620 - __main__ - INFO - Temporary file removed: 4e2169daac2f80d3c75786635a391eb004e4f3f6bb4834f37f3902340fc12985.pdf
2024-12-25 14:50:02,623 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:50:02] "POST /upload HTTP/1.1" 200 -
2024-12-25 14:50:14,682 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.08).
2024-12-25 14:50:14,756 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:50:14] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:50:29,374 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.15).
2024-12-25 14:50:29,399 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:50:29] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:51:09,363 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 14:51:10,363 - werkzeug - INFO -  * Restarting with stat
2024-12-25 14:51:29,392 - __main__ - INFO - Successfully loaded QA model
2024-12-25 14:51:29,398 - __main__ - INFO - Starting application...
2024-12-25 14:51:29,403 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 14:51:29,403 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 14:51:29,403 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 14:51:29,425 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 14:51:29,441 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 14:51:29,579 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:51:29] "GET / HTTP/1.1" 200 -
2024-12-25 14:51:29,921 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:51:29] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 14:51:29,929 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:51:29] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 14:52:01,029 - __main__ - INFO - File saved successfully: 5ece430ec34a1ced3377d5e6c765550b540218304d11931fc0d91862f328e5ae.pdf
2024-12-25 14:52:01,053 - __main__ - INFO - Temporary file removed: 5ece430ec34a1ced3377d5e6c765550b540218304d11931fc0d91862f328e5ae.pdf
2024-12-25 14:52:01,053 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:52:01] "POST /upload HTTP/1.1" 200 -
2024-12-25 14:52:11,485 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.08).
2024-12-25 14:52:11,501 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:52:11] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:52:22,696 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.15).
2024-12-25 14:52:22,718 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:52:22] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:52:38,286 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 14:52:38,929 - werkzeug - INFO -  * Restarting with stat
2024-12-25 14:52:56,883 - __main__ - INFO - Successfully loaded QA model
2024-12-25 14:52:56,891 - __main__ - INFO - Starting application...
2024-12-25 14:52:56,891 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 14:52:56,891 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 14:52:56,891 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 14:52:56,918 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 14:52:56,932 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 14:52:57,123 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:52:57] "GET / HTTP/1.1" 200 -
2024-12-25 14:52:57,509 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:52:57] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 14:52:57,536 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:52:57] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 14:53:07,363 - __main__ - INFO - File saved successfully: c490f702e9f550855b9c30f5103c6dba0e76a136b3909e8b4de0c0dcf02c5191.pdf
2024-12-25 14:53:07,384 - __main__ - INFO - Temporary file removed: c490f702e9f550855b9c30f5103c6dba0e76a136b3909e8b4de0c0dcf02c5191.pdf
2024-12-25 14:53:07,384 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:07] "POST /upload HTTP/1.1" 200 -
2024-12-25 14:53:17,379 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:17] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:17,459 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:17] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:18,614 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:18] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:18,670 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:18] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:24,340 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.08).
2024-12-25 14:53:24,361 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:24] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:24,385 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.08).
2024-12-25 14:53:24,418 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:24] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:38,512 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:38] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:38,553 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:38] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:51,289 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:51] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:53:51,371 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:53:51] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:56:54,455 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 14:56:55,412 - werkzeug - INFO -  * Restarting with stat
2024-12-25 14:57:14,934 - __main__ - INFO - Successfully loaded QA model
2024-12-25 14:57:14,938 - __main__ - INFO - Starting application...
2024-12-25 14:57:14,938 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 14:57:14,938 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 14:57:14,938 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 14:57:14,971 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 14:57:14,987 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 14:57:15,132 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:57:15] "GET / HTTP/1.1" 200 -
2024-12-25 14:57:15,456 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:57:15] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 14:57:15,458 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:57:15] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 14:57:31,061 - __main__ - INFO - File saved successfully: 8bb9f96db248a8fd25315ffb6843733b8afb41eea97d274eda42e70894d233a5.pdf
2024-12-25 14:57:31,085 - __main__ - INFO - Temporary file removed: 8bb9f96db248a8fd25315ffb6843733b8afb41eea97d274eda42e70894d233a5.pdf
2024-12-25 14:57:31,090 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:57:31] "POST /upload HTTP/1.1" 200 -
2024-12-25 14:57:40,545 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:57:40] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:57:40,586 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:57:40] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:58:17,088 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 14:58:18,036 - werkzeug - INFO -  * Restarting with stat
2024-12-25 14:58:35,475 - __main__ - INFO - Successfully loaded QA model
2024-12-25 14:58:35,483 - __main__ - INFO - Starting application...
2024-12-25 14:58:35,489 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 14:58:35,493 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 14:58:35,493 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 14:58:35,516 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 14:58:35,532 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 14:58:35,671 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:58:35] "GET / HTTP/1.1" 200 -
2024-12-25 14:58:36,003 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:58:36] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 14:58:36,043 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:58:36] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 14:58:48,170 - __main__ - INFO - File saved successfully: 9390d7f5faf7557362f366998264a9a3b81fcb37c92d3315972e380f3978572b.pdf
2024-12-25 14:58:48,182 - __main__ - INFO - Temporary file removed: 9390d7f5faf7557362f366998264a9a3b81fcb37c92d3315972e380f3978572b.pdf
2024-12-25 14:58:48,182 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:58:48] "POST /upload HTTP/1.1" 200 -
2024-12-25 14:58:57,006 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.02).
2024-12-25 14:58:57,037 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.02).
2024-12-25 14:58:57,046 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:58:57] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:58:57,080 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:58:57] "POST /ask HTTP/1.1" 200 -
2024-12-25 14:59:11,507 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 14:59:11] "POST /ask HTTP/1.1" 200 -
2024-12-25 15:14:52,128 - __main__ - INFO - Successfully loaded QA model
2024-12-25 15:14:52,134 - __main__ - INFO - Starting application...
2024-12-25 15:14:52,134 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 15:14:52,134 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 15:14:52,134 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 15:14:52,150 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 15:14:52,150 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 15:14:52,150 - werkzeug - INFO -  * Restarting with stat
2024-12-25 15:15:09,286 - __main__ - INFO - Successfully loaded QA model
2024-12-25 15:15:09,286 - __main__ - INFO - Starting application...
2024-12-25 15:15:09,295 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 15:15:09,296 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 15:15:09,297 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 15:15:09,314 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 15:15:09,319 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 15:15:09,523 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 15:15:09] "GET / HTTP/1.1" 200 -
2024-12-25 15:15:09,808 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 15:15:09] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-25 15:15:09,840 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 15:15:09] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 15:15:36,865 - __main__ - INFO - File saved successfully: f659116e3f32d205fcd64e8c0a19bf51eeebeb613ce83e8a2d315ce56ffc6e6d.pdf
2024-12-25 15:15:36,889 - __main__ - INFO - Temporary file removed: f659116e3f32d205fcd64e8c0a19bf51eeebeb613ce83e8a2d315ce56ffc6e6d.pdf
2024-12-25 15:15:36,890 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 15:15:36] "POST /upload HTTP/1.1" 200 -
2024-12-25 15:15:53,523 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.08).
2024-12-25 15:15:53,529 - __main__ - WARNING - Warning: The answer may be inconsistent with the supporting text (Similarity Score: 0.08).
2024-12-25 15:15:53,564 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 15:15:53] "POST /ask HTTP/1.1" 200 -
2024-12-25 15:15:53,564 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 15:15:53] "POST /ask HTTP/1.1" 200 -
2024-12-25 15:16:08,686 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 15:16:08] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:11:03,080 - __main__ - INFO - Successfully loaded summarization model
2024-12-25 19:11:03,089 - __main__ - INFO - Starting application...
2024-12-25 19:11:03,089 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:11:03,089 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:11:03,090 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:11:03,157 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 19:11:03,158 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 19:11:03,163 - werkzeug - INFO -  * Restarting with stat
2024-12-25 19:11:25,169 - __main__ - INFO - Successfully loaded summarization model
2024-12-25 19:11:25,171 - __main__ - INFO - Starting application...
2024-12-25 19:11:25,171 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:11:25,172 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:11:25,172 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:11:25,180 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 19:11:25,187 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 19:11:25,273 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:11:25] "GET / HTTP/1.1" 200 -
2024-12-25 19:11:25,502 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:11:25] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-25 19:11:25,503 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:11:25] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 19:11:34,781 - __main__ - INFO - File saved successfully: ee6333e18c8005f5adfaaa455fb4c8f62f402c955f3810608d554a61a124f833.pdf
2024-12-25 19:11:34,821 - __main__ - INFO - Temporary file removed: ee6333e18c8005f5adfaaa455fb4c8f62f402c955f3810608d554a61a124f833.pdf
2024-12-25 19:11:34,821 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:11:34] "POST /upload HTTP/1.1" 200 -
2024-12-25 19:11:46,578 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:11:46] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:11:57,165 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:11:57] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:11:57,183 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:11:57] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:12:32,200 - __main__ - INFO - File saved successfully: c81e30e6d0e6bf43308b10cc8f292582f495e0f23ac130ca26a7a96ae50afd52.pdf
2024-12-25 19:12:32,205 - __main__ - INFO - Temporary file removed: c81e30e6d0e6bf43308b10cc8f292582f495e0f23ac130ca26a7a96ae50afd52.pdf
2024-12-25 19:12:32,206 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:12:32] "POST /upload HTTP/1.1" 200 -
2024-12-25 19:12:33,758 - __main__ - INFO - File saved successfully: 29663f1e767aba98faccf0665235b9ca64ee266cf5781899c2baea439ac1e759.pdf
2024-12-25 19:12:33,768 - __main__ - INFO - Temporary file removed: 29663f1e767aba98faccf0665235b9ca64ee266cf5781899c2baea439ac1e759.pdf
2024-12-25 19:12:33,768 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:12:33] "POST /upload HTTP/1.1" 200 -
2024-12-25 19:12:45,130 - __main__ - INFO - File saved successfully: 4c7b8b014fae8208d1ef9051e157de1c1f03baf1ef167e9dab4853f17c81b1ab.pdf
2024-12-25 19:12:45,295 - __main__ - INFO - Temporary file removed: 4c7b8b014fae8208d1ef9051e157de1c1f03baf1ef167e9dab4853f17c81b1ab.pdf
2024-12-25 19:12:45,296 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:12:45] "POST /upload HTTP/1.1" 200 -
2024-12-25 19:13:03,684 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:13:03] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:13:03,702 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:13:03] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:13:31,982 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:13:31] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:13:32,020 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:13:32] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:13:48,122 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:13:48] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:13:48,231 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:13:48] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:14:02,733 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:14:02] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:14:02,734 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:14:02] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:14:24,132 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:14:24] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:25:13,704 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 19:25:16,818 - werkzeug - INFO -  * Restarting with stat
2024-12-25 19:26:42,050 - __main__ - INFO - Successfully loaded QA model
2024-12-25 19:26:42,051 - __main__ - INFO - Starting application...
2024-12-25 19:26:42,051 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:26:42,051 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:26:42,051 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:26:42,057 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 19:26:42,059 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 19:27:13,543 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:27:13] "[33mGET / HTTP/1.1[0m" 404 -
2024-12-25 19:27:31,442 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:27:31] "[33mGET / HTTP/1.1[0m" 404 -
2024-12-25 19:27:33,067 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:27:33] "[33mGET / HTTP/1.1[0m" 404 -
2024-12-25 19:28:59,655 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 19:29:00,418 - werkzeug - INFO -  * Restarting with stat
2024-12-25 19:29:14,983 - __main__ - INFO - Successfully loaded summarization model
2024-12-25 19:29:15,004 - __main__ - INFO - Starting application...
2024-12-25 19:29:15,004 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:29:15,005 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:29:15,005 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:29:15,093 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 19:29:15,106 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 19:29:20,892 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:29:20] "GET / HTTP/1.1" 200 -
2024-12-25 19:29:21,116 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:29:21] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 19:29:21,117 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:29:21] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-25 19:29:28,188 - __main__ - INFO - File saved successfully: cb39e94ca871a4a13127626972535d0199b1aeda63ba0fafb660a926b4b8e540.pdf
2024-12-25 19:29:28,232 - __main__ - INFO - Temporary file removed: cb39e94ca871a4a13127626972535d0199b1aeda63ba0fafb660a926b4b8e540.pdf
2024-12-25 19:29:28,234 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:29:28] "POST /upload HTTP/1.1" 200 -
2024-12-25 19:29:56,924 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:29:56] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:29:57,332 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:29:57] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:30:36,197 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:30:36] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:30:36,632 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:30:36] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:32:10,918 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 19:32:12,643 - werkzeug - INFO -  * Restarting with stat
2024-12-25 19:32:23,482 - __main__ - INFO - Successfully loaded QA model
2024-12-25 19:32:23,485 - __main__ - INFO - Starting application...
2024-12-25 19:32:23,485 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:32:23,487 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:32:23,487 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:32:23,497 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 19:32:23,500 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 19:32:35,624 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:32:35] "GET / HTTP/1.1" 200 -
2024-12-25 19:32:35,734 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:32:35] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 19:32:35,734 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:32:35] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 19:32:42,246 - __main__ - INFO - File saved successfully: 3e3c5177e97e3504f8252503eb8568a6cb885e2b1f1bd6810614c5c4c9799df2.pdf
2024-12-25 19:32:42,268 - __main__ - INFO - Temporary file removed: 3e3c5177e97e3504f8252503eb8568a6cb885e2b1f1bd6810614c5c4c9799df2.pdf
2024-12-25 19:32:42,269 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:32:42] "POST /upload HTTP/1.1" 200 -
2024-12-25 19:32:46,556 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:32:46] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:32:46,557 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:32:46] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:32:56,382 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:32:56] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:32:56,383 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:32:56] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:33:03,398 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:03] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:33:03,402 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:03] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:33:09,906 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:09] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:33:09,912 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:09] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:33:24,736 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:24] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:33:24,739 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:24] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:33:35,148 - __main__ - INFO - File saved successfully: ce6d9d770bbb2f8061b79a84144e96a4fae24ed6230ee9c146765111b91ad857.pdf
2024-12-25 19:33:35,918 - __main__ - INFO - Temporary file removed: ce6d9d770bbb2f8061b79a84144e96a4fae24ed6230ee9c146765111b91ad857.pdf
2024-12-25 19:33:35,921 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:35] "POST /upload HTTP/1.1" 200 -
2024-12-25 19:33:58,270 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:58] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:33:58,422 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:33:58] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:34:09,328 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:34:09] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:34:09,376 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:34:09] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:34:38,067 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:34:38] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:34:38,103 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:34:38] "POST /ask HTTP/1.1" 200 -
2024-12-25 19:37:05,437 - __main__ - INFO - Starting application...
2024-12-25 19:37:05,437 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:37:05,438 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:37:05,438 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:37:05,537 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 19:37:05,538 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 19:37:05,540 - werkzeug - INFO -  * Restarting with stat
2024-12-25 19:37:06,241 - __main__ - INFO - Starting application...
2024-12-25 19:37:06,241 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:37:06,242 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:37:06,242 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:37:06,249 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 19:37:06,250 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 19:37:09,761 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:37:09] "GET / HTTP/1.1" 200 -
2024-12-25 19:37:09,824 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:37:09] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 19:37:09,824 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:37:09] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 19:37:14,483 - __main__ - INFO - File saved successfully: ebbdb0a83a022b99fed48a83823bbbfeb723bfef7c7cbf481f4a82bb0a989bbd.pdf
2024-12-25 19:37:14,503 - __main__ - INFO - Temporary file removed: ebbdb0a83a022b99fed48a83823bbbfeb723bfef7c7cbf481f4a82bb0a989bbd.pdf
2024-12-25 19:37:14,504 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:37:14] "POST /upload HTTP/1.1" 200 -
2024-12-25 19:37:23,073 - __main__ - ERROR - Model error: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/question-answering (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000244BF1058B0>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))
2024-12-25 19:37:23,074 - __main__ - ERROR - Model error: HTTPSConnectionPool(host='api.ollama.com', port=443): Max retries exceeded with url: /v1/question-answering (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x00000244BF104B30>: Failed to resolve 'api.ollama.com' ([Errno 11001] getaddrinfo failed)"))
2024-12-25 19:37:23,076 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:37:23] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-25 19:37:23,078 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 19:37:23] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-25 19:42:13,246 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 19:42:13,391 - werkzeug - INFO -  * Restarting with stat
2024-12-25 19:56:44,717 - __main__ - INFO - Starting application...
2024-12-25 19:56:44,719 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:56:44,719 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:56:44,719 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:56:46,760 - __main__ - WARNING - Ollama service is not available. Please ensure Ollama is running and the mistral model is installed.
2024-12-25 19:56:46,760 - __main__ - INFO - To install Ollama, visit: https://ollama.ai/download
2024-12-25 19:56:46,760 - __main__ - INFO - After installation, run: 'ollama pull mistral' to download the required model
2024-12-25 19:56:46,783 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 19:56:46,783 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 19:56:46,786 - werkzeug - INFO -  * Restarting with stat
2024-12-25 19:57:03,431 - __main__ - INFO - Starting application...
2024-12-25 19:57:03,432 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:57:03,432 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:57:03,432 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:57:05,460 - __main__ - WARNING - Ollama service is not available. Please ensure Ollama is running and the mistral model is installed.
2024-12-25 19:57:05,460 - __main__ - INFO - To install Ollama, visit: https://ollama.ai/download
2024-12-25 19:57:05,460 - __main__ - INFO - After installation, run: 'ollama pull mistral' to download the required model
2024-12-25 19:57:05,466 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 19:57:05,466 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 19:59:21,677 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 19:59:22,727 - werkzeug - INFO -  * Restarting with stat
2024-12-25 19:59:37,306 - __main__ - INFO - Starting application...
2024-12-25 19:59:39,351 - __main__ - WARNING - Ollama service issue: Mistral model not found. Please run 'ollama pull mistral'
2024-12-25 19:59:39,351 - __main__ - INFO - To install Ollama, visit: https://ollama.ai/download
2024-12-25 19:59:39,351 - __main__ - INFO - After installation, run: 'ollama pull mistral' to download the required model
2024-12-25 19:59:39,352 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 19:59:39,352 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 19:59:39,352 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 19:59:39,357 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 19:59:39,360 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 20:09:35,699 - __main__ - INFO - Starting application...
2024-12-25 20:09:37,743 - __main__ - WARNING - Ollama service issue: Mistral model not found. Please run 'ollama pull mistral'
2024-12-25 20:09:37,744 - __main__ - INFO - To install Ollama, visit: https://ollama.ai/download
2024-12-25 20:09:37,744 - __main__ - INFO - After installation, run: 'ollama pull mistral' to download the required model
2024-12-25 20:09:37,744 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 20:09:37,744 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 20:09:37,744 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 20:09:37,769 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 20:09:37,769 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 20:09:37,773 - werkzeug - INFO -  * Restarting with stat
2024-12-25 20:09:55,816 - __main__ - INFO - Starting application...
2024-12-25 20:09:57,867 - __main__ - WARNING - Ollama service issue: Mistral model not found. Please run 'ollama pull mistral'
2024-12-25 20:09:57,868 - __main__ - INFO - To install Ollama, visit: https://ollama.ai/download
2024-12-25 20:09:57,868 - __main__ - INFO - After installation, run: 'ollama pull mistral' to download the required model
2024-12-25 20:09:57,868 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 20:09:57,868 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 20:09:57,869 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 20:09:57,874 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 20:09:57,886 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 20:09:57,985 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:09:57] "GET / HTTP/1.1" 200 -
2024-12-25 20:09:58,264 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:09:58] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 20:09:58,270 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:09:58] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 20:10:04,553 - __main__ - INFO - File saved successfully: 09f8d861b8e76f6d19d1ae6427448c855788b07b204aec62c38a4c9363824677.pdf
2024-12-25 20:10:04,567 - __main__ - INFO - Temporary file removed: 09f8d861b8e76f6d19d1ae6427448c855788b07b204aec62c38a4c9363824677.pdf
2024-12-25 20:10:04,567 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:10:04] "POST /upload HTTP/1.1" 200 -
2024-12-25 20:10:12,654 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:10:12] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:10:12,656 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:10:12] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:14:01,954 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 20:14:04,855 - werkzeug - INFO -  * Restarting with stat
2024-12-25 20:14:20,157 - __main__ - INFO - Starting application...
2024-12-25 20:14:20,157 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:14:32,198 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:14:32,220 - __main__ - WARNING - Ollama service issue: Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:14:32,222 - __main__ - INFO - To install Ollama, visit: https://ollama.ai/download
2024-12-25 20:14:32,223 - __main__ - INFO - After installation, run: 'ollama pull mistral' to download the required model
2024-12-25 20:14:32,224 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 20:14:32,225 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 20:14:32,225 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 20:14:32,263 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 20:14:32,283 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 20:16:06,168 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 20:16:08,034 - werkzeug - INFO -  * Restarting with stat
2024-12-25 20:16:23,904 - __main__ - INFO - Starting application...
2024-12-25 20:16:23,904 - __main__ - INFO - Verifying Ollama setup...
2024-12-25 20:16:26,011 - __main__ - INFO - Ollama service status: 200
2024-12-25 20:16:26,012 - __main__ - INFO - Available models: [{'name': 'mistral:latest', 'model': 'mistral:latest', 'modified_at': '2024-12-25T20:10:29.7226198+05:30', 'size': 4113301824, 'digest': 'f974a74358d62a017b37c6f424fcdf2744ca02926c4f952513ddf474b2fa5091', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '7.2B', 'quantization_level': 'Q4_0'}}]
2024-12-25 20:16:55,383 - __main__ - INFO - Successfully tested Mistral model
2024-12-25 20:16:55,398 - __main__ - INFO - Ollama setup verified successfully
2024-12-25 20:16:55,399 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 20:16:55,402 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 20:16:55,402 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 20:16:55,496 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 20:16:55,532 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 20:16:55,983 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:16:55] "GET / HTTP/1.1" 200 -
2024-12-25 20:16:56,409 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:16:56] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 20:16:56,411 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:16:56] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 20:17:05,582 - __main__ - INFO - File saved successfully: d5b8cedc4743ef4a69ee918ce29f09a40be1f6762b018af9fe767517a6f87cbe.pdf
2024-12-25 20:17:05,645 - __main__ - INFO - Temporary file removed: d5b8cedc4743ef4a69ee918ce29f09a40be1f6762b018af9fe767517a6f87cbe.pdf
2024-12-25 20:17:05,645 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:17:05] "POST /upload HTTP/1.1" 200 -
2024-12-25 20:17:11,304 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:17:11,321 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:17:23,425 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:17:23,426 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:17:23,427 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:17:23] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:17:23,427 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:17:23] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:17:43,929 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:17:43,932 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:17:54,624 - __main__ - INFO - Health check response status: 200
2024-12-25 20:17:54,624 - __main__ - INFO - Ollama is healthy and responding
2024-12-25 20:17:54,947 - __main__ - INFO - Attempting to connect to Ollama...
2024-12-25 20:17:55,974 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:17:55,976 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:17:55] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:18:03,164 - __main__ - INFO - Ollama response status: 200
2024-12-25 20:18:03,164 - __main__ - INFO - Successfully received response from Ollama
2024-12-25 20:18:04,586 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:18:04] "POST /ask HTTP/1.1" 200 -
2024-12-25 20:18:20,522 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:18:20,522 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:18:30,134 - __main__ - INFO - Health check response status: 200
2024-12-25 20:18:30,134 - __main__ - INFO - Ollama is healthy and responding
2024-12-25 20:18:30,162 - __main__ - INFO - Attempting to connect to Ollama...
2024-12-25 20:18:30,504 - __main__ - INFO - Health check response status: 200
2024-12-25 20:18:30,504 - __main__ - INFO - Ollama is healthy and responding
2024-12-25 20:18:30,533 - __main__ - INFO - Attempting to connect to Ollama...
2024-12-25 20:18:48,284 - __main__ - INFO - Ollama response status: 200
2024-12-25 20:18:48,285 - __main__ - INFO - Ollama response status: 200
2024-12-25 20:18:48,285 - __main__ - INFO - Successfully received response from Ollama
2024-12-25 20:18:48,286 - __main__ - INFO - Successfully received response from Ollama
2024-12-25 20:18:48,367 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:18:48] "POST /ask HTTP/1.1" 200 -
2024-12-25 20:18:48,367 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:18:48] "POST /ask HTTP/1.1" 200 -
2024-12-25 20:19:35,082 - __main__ - INFO - File saved successfully: 3465e714435591f0a75a91cdf852458536bf8a68c8217e416ccc2a34bef0722a.pdf
2024-12-25 20:19:35,305 - __main__ - INFO - Temporary file removed: 3465e714435591f0a75a91cdf852458536bf8a68c8217e416ccc2a34bef0722a.pdf
2024-12-25 20:19:35,305 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:19:35] "POST /upload HTTP/1.1" 200 -
2024-12-25 20:20:03,578 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:20:15,620 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:20:15,624 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:20:15] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:20:46,369 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:20:46,371 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:20:58,422 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:20:58,424 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:20:58] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:20:58,438 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:20:58,440 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:20:58] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:21:19,713 - __main__ - INFO - File saved successfully: 5d34298bfb6362a43f4318dd3f782744adff37aa37737deaa3759785a4fbb33f.pdf
2024-12-25 20:21:19,743 - __main__ - INFO - Temporary file removed: 5d34298bfb6362a43f4318dd3f782744adff37aa37737deaa3759785a4fbb33f.pdf
2024-12-25 20:21:19,745 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:21:19] "POST /upload HTTP/1.1" 200 -
2024-12-25 20:21:22,659 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:21:34,680 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:21:34,683 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:21:34] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:21:49,415 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:21:49,422 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:22:00,383 - __main__ - INFO - Health check response status: 200
2024-12-25 20:22:00,383 - __main__ - INFO - Ollama is healthy and responding
2024-12-25 20:22:00,566 - __main__ - INFO - Attempting to connect to Ollama...
2024-12-25 20:22:01,469 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:22:01,470 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:22:01] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:22:11,282 - __main__ - INFO - Ollama response status: 200
2024-12-25 20:22:11,285 - __main__ - INFO - Successfully received response from Ollama
2024-12-25 20:22:11,852 - __main__ - WARNING - Hallucinated entities detected: {'France'}
2024-12-25 20:22:11,854 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:22:11] "POST /ask HTTP/1.1" 200 -
2024-12-25 20:22:37,496 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:22:37,498 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:22:47,903 - __main__ - INFO - Health check response status: 200
2024-12-25 20:22:47,903 - __main__ - INFO - Ollama is healthy and responding
2024-12-25 20:22:47,940 - __main__ - INFO - Attempting to connect to Ollama...
2024-12-25 20:22:49,577 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:22:49,578 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:22:49] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:22:58,447 - __main__ - INFO - Ollama response status: 200
2024-12-25 20:22:58,447 - __main__ - INFO - Successfully received response from Ollama
2024-12-25 20:22:58,603 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:22:58] "POST /ask HTTP/1.1" 200 -
2024-12-25 20:23:27,722 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:23:27,734 - __main__ - INFO - Checking Ollama health...
2024-12-25 20:23:39,767 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:23:39,773 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:23:39] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:23:39,783 - __main__ - ERROR - Unexpected error checking Ollama health: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=10)
2024-12-25 20:23:39,811 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 20:23:39] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 20:45:57,493 - __main__ - INFO - Loading NLP models...
2024-12-25 20:45:57,979 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 20:45:57,979 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 20:46:01,303 - __main__ - INFO - NLP models loaded successfully
2024-12-25 20:46:01,344 - __main__ - INFO - Starting application...
2024-12-25 20:46:01,344 - __main__ - INFO - Warming up Ollama...
2024-12-25 20:46:01,344 - __main__ - INFO - Sending request to Ollama...
2024-12-25 20:46:33,452 - __main__ - ERROR - Ollama request timed out
2024-12-25 20:46:34,476 - __main__ - INFO - Sending request to Ollama...
2024-12-25 20:47:06,640 - __main__ - ERROR - Ollama request timed out
2024-12-25 20:47:08,646 - __main__ - INFO - Sending request to Ollama...
2024-12-25 20:47:15,313 - __main__ - ERROR - Connection to Ollama failed
2024-12-25 20:47:18,314 - __main__ - INFO - Ollama warmup completed
2024-12-25 20:47:18,316 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 20:47:18,320 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 20:47:18,320 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 20:47:18,713 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 20:47:18,713 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 20:47:18,730 - werkzeug - INFO -  * Restarting with stat
2024-12-25 20:47:30,435 - __main__ - INFO - Loading NLP models...
2024-12-25 20:47:31,080 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 20:47:31,080 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 20:47:34,590 - __main__ - INFO - NLP models loaded successfully
2024-12-25 20:47:34,602 - __main__ - INFO - Starting application...
2024-12-25 20:47:34,602 - __main__ - INFO - Warming up Ollama...
2024-12-25 20:47:34,604 - __main__ - INFO - Sending request to Ollama...
2024-12-25 20:51:18,494 - __main__ - INFO - Loading NLP models...
2024-12-25 20:51:18,997 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 20:51:18,997 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 20:51:23,785 - __main__ - INFO - NLP models loaded successfully
2024-12-25 20:51:23,792 - __main__ - INFO - Starting application...
2024-12-25 20:51:23,792 - __main__ - INFO - Warming up Ollama...
2024-12-25 20:51:23,792 - __main__ - INFO - Sending request to Ollama...
2024-12-25 20:51:53,809 - __main__ - ERROR - Ollama request timed out
2024-12-25 20:51:54,823 - __main__ - INFO - Sending request to Ollama...
2024-12-25 20:52:24,856 - __main__ - ERROR - Ollama request timed out
2024-12-25 20:52:26,858 - __main__ - INFO - Sending request to Ollama...
2024-12-25 20:52:56,883 - __main__ - ERROR - Ollama request timed out
2024-12-25 20:52:59,886 - __main__ - INFO - Ollama warmup completed
2024-12-25 20:52:59,889 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 20:52:59,891 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 20:52:59,891 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 20:53:00,150 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 20:53:00,150 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 20:53:00,166 - werkzeug - INFO -  * Restarting with stat
2024-12-25 20:53:12,447 - __main__ - INFO - Loading NLP models...
2024-12-25 20:53:13,169 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 20:53:13,169 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 20:53:17,726 - __main__ - INFO - NLP models loaded successfully
2024-12-25 20:53:17,751 - __main__ - INFO - Starting application...
2024-12-25 20:53:17,753 - __main__ - INFO - Warming up Ollama...
2024-12-25 20:53:17,753 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:00:18,588 - __main__ - INFO - Loading NLP models...
2024-12-25 21:00:19,230 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 21:00:19,230 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 21:00:23,862 - __main__ - INFO - NLP models loaded successfully
2024-12-25 21:00:23,871 - __main__ - INFO - Starting application...
2024-12-25 21:00:23,871 - __main__ - INFO - Warming up Ollama...
2024-12-25 21:00:23,871 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:00:26,813 - __main__ - INFO - Ollama warmup completed successfully
2024-12-25 21:00:26,814 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:00:26,814 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:00:26,814 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:00:26,834 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 21:00:26,839 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 21:00:26,841 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:00:34,654 - __main__ - INFO - Loading NLP models...
2024-12-25 21:00:35,134 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 21:00:35,134 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 21:00:39,254 - __main__ - INFO - NLP models loaded successfully
2024-12-25 21:00:39,266 - __main__ - INFO - Starting application...
2024-12-25 21:00:39,266 - __main__ - INFO - Warming up Ollama...
2024-12-25 21:00:39,266 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:00:46,381 - __main__ - INFO - Ollama warmup completed successfully
2024-12-25 21:00:46,381 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:00:46,384 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:00:46,384 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:00:46,385 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:00:46,394 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:00:46,454 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:00:46] "GET / HTTP/1.1" 200 -
2024-12-25 21:00:46,610 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:00:46] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:00:46,614 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:00:46] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:00:58,785 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:00:58] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:01:03,194 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:03,195 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:14,148 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:01:14] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:01:14,148 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:01:14] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:01:15,027 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:17,167 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:30,039 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:01:30,541 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:32,175 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:01:32,677 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:35,154 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:35,194 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:42,698 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:01:42] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:01:44,716 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:01:44] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:01:45,559 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:01:46,560 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:47,704 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:01:48,705 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:01:56,190 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:01:56] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:02:01,572 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:02:03,075 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:02:03] "[35m[1mGET /test-ollama HTTP/1.1[0m" 500 -
2024-12-25 21:02:03,709 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:02:05,211 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:02:05] "[35m[1mGET /test-ollama HTTP/1.1[0m" 500 -
2024-12-25 21:02:06,150 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:02:06] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2024-12-25 21:02:09,351 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:02:09,366 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:02:24,364 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:02:24,372 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:02:24,865 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:02:24,874 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:02:39,871 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:02:39,895 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:02:40,872 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:02:40,896 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:02:55,882 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:02:55,914 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:02:57,391 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:02:57] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:02:57,417 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:02:57] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:04:47,655 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 21:04:49,288 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:04:57,157 - __main__ - INFO - Loading NLP models...
2024-12-25 21:04:57,577 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 21:04:57,577 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 21:05:01,312 - __main__ - INFO - NLP models loaded successfully
2024-12-25 21:05:01,321 - __main__ - INFO - Starting application...
2024-12-25 21:05:01,321 - __main__ - INFO - Warming up Ollama...
2024-12-25 21:05:01,321 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:05:16,342 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:05:16,856 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:05:31,927 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:05:32,928 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:05:47,937 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:05:49,438 - __main__ - INFO - Ollama warmup completed successfully
2024-12-25 21:05:49,440 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:05:49,442 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:05:49,443 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:05:49,564 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:05:49,605 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:06:30,959 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:06:30] "GET / HTTP/1.1" 200 -
2024-12-25 21:06:31,682 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:06:31] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:06:31,684 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:06:31] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:06:43,795 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:06:43] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:06:53,591 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:06:53,594 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:07:08,607 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:07:08,608 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:07:09,109 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:07:09,109 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:07:24,114 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:07:24,115 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:07:25,116 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:07:25,116 - __main__ - INFO - Sending request to Ollama...
2024-12-25 21:07:40,118 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:07:40,119 - __main__ - ERROR - Ollama request timed out
2024-12-25 21:07:41,629 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:07:41] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:07:41,630 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:07:41] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:09:01,879 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 21:09:05,614 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:09:19,255 - __main__ - INFO - Loading NLP models...
2024-12-25 21:09:19,877 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 21:09:19,881 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 21:09:23,414 - __main__ - INFO - NLP models loaded successfully
2024-12-25 21:09:23,426 - __main__ - INFO - Starting application...
2024-12-25 21:09:23,426 - __main__ - INFO - Warming up Ollama...
2024-12-25 21:09:28,875 - __main__ - INFO - Ollama warmup completed successfully
2024-12-25 21:09:28,875 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:09:28,876 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:09:28,876 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:09:28,884 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:09:28,890 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:09:44,946 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:09:44] "GET / HTTP/1.1" 200 -
2024-12-25 21:09:45,085 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:09:45] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:09:45,085 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:09:45] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:09:55,343 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:09:55] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:10:20,429 - __main__ - ERROR - Request timed out
2024-12-25 21:10:20,429 - __main__ - ERROR - Request timed out
2024-12-25 21:10:35,934 - __main__ - ERROR - Request timed out
2024-12-25 21:10:35,934 - __main__ - ERROR - Request timed out
2024-12-25 21:10:49,384 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:10:49] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:10:50,536 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:10:50] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:11:21,136 - __main__ - ERROR - Request timed out
2024-12-25 21:11:21,144 - __main__ - ERROR - Request timed out
2024-12-25 21:11:36,656 - __main__ - ERROR - Request timed out
2024-12-25 21:11:36,656 - __main__ - ERROR - Request timed out
2024-12-25 21:11:52,173 - __main__ - ERROR - Request timed out
2024-12-25 21:11:52,173 - __main__ - ERROR - Request timed out
2024-12-25 21:12:07,677 - __main__ - ERROR - Request timed out
2024-12-25 21:12:07,685 - __main__ - ERROR - Request timed out
2024-12-25 21:12:22,802 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:12:22] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:12:23,196 - __main__ - ERROR - Request timed out
2024-12-25 21:12:33,589 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:12:33] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:12:59,399 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:12:59] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:13:01,486 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:13:01] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:13:18,846 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:13:18] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:13:28,150 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:13:28] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:13:31,907 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:13:31] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:13:44,629 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:13:44] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:13:44,783 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:13:44] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:13:54,487 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:13:54] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:13:57,927 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:13:57] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:14:11,181 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:14:11] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:14:35,968 - __main__ - ERROR - Request timed out
2024-12-25 21:14:36,017 - __main__ - ERROR - Request timed out
2024-12-25 21:14:51,475 - __main__ - ERROR - Request timed out
2024-12-25 21:14:51,524 - __main__ - ERROR - Request timed out
2024-12-25 21:15:06,983 - __main__ - ERROR - Request timed out
2024-12-25 21:15:07,032 - __main__ - ERROR - Request timed out
2024-12-25 21:15:22,489 - __main__ - ERROR - Request timed out
2024-12-25 21:15:22,537 - __main__ - ERROR - Request timed out
2024-12-25 21:15:29,884 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 21:15:31,544 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:15:42,834 - __main__ - INFO - Loading NLP models...
2024-12-25 21:15:43,514 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 21:15:43,514 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2024-12-25 21:15:48,759 - __main__ - INFO - NLP models loaded successfully
2024-12-25 21:15:48,764 - __main__ - INFO - Starting application...
2024-12-25 21:15:48,764 - __main__ - INFO - Warming up Ollama...
2024-12-25 21:15:48,774 - __main__ - ERROR - Ollama error response: {"error":"model 'phi' not found"}
2024-12-25 21:15:49,281 - __main__ - ERROR - Ollama error response: {"error":"model 'phi' not found"}
2024-12-25 21:15:49,789 - __main__ - ERROR - Ollama error response: {"error":"model 'phi' not found"}
2024-12-25 21:15:50,291 - __main__ - INFO - Ollama warmup completed successfully
2024-12-25 21:15:50,291 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:15:50,291 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:15:50,292 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:15:50,357 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:15:50,357 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:27:15,106 - __main__ - INFO - Loading QA model...
2024-12-25 21:27:15,106 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:27:25,926 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:27:25,926 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:27:25,936 - __main__ - INFO - Starting application...
2024-12-25 21:27:25,961 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 21:27:25,961 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 21:27:25,966 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:27:31,021 - __main__ - INFO - Loading QA model...
2024-12-25 21:27:31,021 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:27:35,986 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:27:35,990 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:27:35,998 - __main__ - INFO - Starting application...
2024-12-25 21:27:36,018 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:27:36,030 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:27:36,408 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:27:36] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-12-25 21:27:36,764 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:27:36] "GET /?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1" 200 -
2024-12-25 21:27:36,764 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:27:36] "GET /?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1" 200 -
2024-12-25 21:27:36,922 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:27:36] "GET /?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1" 200 -
2024-12-25 21:27:36,937 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:27:36] "[36mGET /?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1[0m" 304 -
2024-12-25 21:32:17,226 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 21:32:17,867 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:32:22,031 - __main__ - INFO - Loading QA model...
2024-12-25 21:32:22,031 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:32:25,599 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:32:25,599 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:32:25,606 - __main__ - INFO - Starting application...
2024-12-25 21:32:25,606 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 21:32:25,606 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:32:25,606 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:32:25,606 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:32:25,618 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:32:25,622 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:32:46,767 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:32:46] "GET / HTTP/1.1" 200 -
2024-12-25 21:32:46,889 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:32:46] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:32:46,890 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:32:46] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:32:54,847 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:32:54] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:33:20,117 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:33:20] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:33:20,120 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:33:20] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:33:30,287 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:33:30] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:33:30,287 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:33:30] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:33:53,727 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:33:53] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:33:53,730 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:33:53] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:34:11,942 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:34:11] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:34:19,640 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:34:19] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:34:19,640 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:34:19] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:34:34,101 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:34:34] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:34:34,103 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:34:34] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:37:32,216 - __main__ - INFO - Loading QA model...
2024-12-25 21:37:32,216 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:37:33,597 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:37:33,597 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:37:33,600 - __main__ - INFO - Starting application...
2024-12-25 21:37:33,600 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 21:37:33,600 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:37:33,607 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:37:33,607 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:37:33,618 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.16.72:5000
2024-12-25 21:37:33,627 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 21:37:33,630 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:37:40,618 - __main__ - INFO - Loading QA model...
2024-12-25 21:37:40,618 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:37:42,787 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:37:42,787 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:37:42,800 - __main__ - INFO - Starting application...
2024-12-25 21:37:42,800 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 21:37:42,802 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:37:42,802 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:37:42,802 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:37:42,820 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:37:42,835 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:37:42,951 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:37:42] "GET / HTTP/1.1" 200 -
2024-12-25 21:37:43,117 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:37:43] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:37:43,126 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:37:43] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:37:49,847 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:37:49] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:37:52,997 - __main__ - ERROR - Answer generation error: 
**********************************************************************
  Resource [93mpunkt_tab[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('punkt_tab')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtokenizers/punkt_tab/english/[0m

  Searched in:
    - 'C:\\Users\\nanda/nltk_data'
    - 'C:\\Python312\\nltk_data'
    - 'C:\\Python312\\share\\nltk_data'
    - 'C:\\Python312\\lib\\nltk_data'
    - 'C:\\Users\\nanda\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

2024-12-25 21:37:52,997 - __main__ - ERROR - Answer generation error: 
**********************************************************************
  Resource [93mpunkt_tab[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('punkt_tab')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtokenizers/punkt_tab/english/[0m

  Searched in:
    - 'C:\\Users\\nanda/nltk_data'
    - 'C:\\Python312\\nltk_data'
    - 'C:\\Python312\\share\\nltk_data'
    - 'C:\\Python312\\lib\\nltk_data'
    - 'C:\\Users\\nanda\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

2024-12-25 21:37:53,007 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:37:53] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:37:53,014 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:37:53] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:38:42,557 - __main__ - INFO - Loading QA model...
2024-12-25 21:38:42,557 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:38:43,993 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:38:43,993 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:38:44,001 - __main__ - INFO - Starting application...
2024-12-25 21:38:44,001 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 21:38:44,001 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:38:44,006 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:38:44,006 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:38:44,018 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.16.72:5000
2024-12-25 21:38:44,027 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 21:38:44,029 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:38:54,246 - __main__ - INFO - Loading QA model...
2024-12-25 21:38:54,247 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:38:55,155 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:38:55,155 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:38:55,157 - __main__ - INFO - Starting application...
2024-12-25 21:38:55,157 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 21:38:55,157 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:38:55,157 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:38:55,157 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:38:55,169 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:38:55,174 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:38:55,227 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:38:55] "GET / HTTP/1.1" 200 -
2024-12-25 21:38:55,337 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:38:55] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:38:55,365 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:38:55] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:39:01,172 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:39:01] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:39:04,737 - __main__ - ERROR - Answer generation error: 
**********************************************************************
  Resource [93mpunkt_tab[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('punkt_tab')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtokenizers/punkt_tab/english/[0m

  Searched in:
    - 'C:\\Users\\nanda/nltk_data'
    - 'C:\\Python312\\nltk_data'
    - 'C:\\Python312\\share\\nltk_data'
    - 'C:\\Python312\\lib\\nltk_data'
    - 'C:\\Users\\nanda\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

2024-12-25 21:39:04,737 - __main__ - ERROR - Answer generation error: 
**********************************************************************
  Resource [93mpunkt_tab[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('punkt_tab')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtokenizers/punkt_tab/english/[0m

  Searched in:
    - 'C:\\Users\\nanda/nltk_data'
    - 'C:\\Python312\\nltk_data'
    - 'C:\\Python312\\share\\nltk_data'
    - 'C:\\Python312\\lib\\nltk_data'
    - 'C:\\Users\\nanda\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

2024-12-25 21:39:04,737 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:39:04] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:39:04,747 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:39:04] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:39:50,827 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 21:39:51,779 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:40:02,217 - __main__ - INFO - Loading QA model...
2024-12-25 21:40:02,217 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:40:03,448 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:40:03,448 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:40:03,457 - __main__ - INFO - Starting application...
2024-12-25 21:40:03,457 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 21:40:03,457 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:40:03,457 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:40:03,464 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:40:03,477 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:40:03,483 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:40:03,570 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:40:03] "GET / HTTP/1.1" 200 -
2024-12-25 21:40:03,756 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:40:03] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:40:03,759 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:40:03] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:40:11,568 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:40:11] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:40:14,604 - __main__ - ERROR - Answer generation error: 
**********************************************************************
  Resource [93mpunkt_tab[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('punkt_tab')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtokenizers/punkt_tab/english/[0m

  Searched in:
    - 'C:\\Users\\nanda/nltk_data'
    - 'C:\\Python312\\nltk_data'
    - 'C:\\Python312\\share\\nltk_data'
    - 'C:\\Python312\\lib\\nltk_data'
    - 'C:\\Users\\nanda\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

2024-12-25 21:40:14,607 - __main__ - ERROR - Answer generation error: 
**********************************************************************
  Resource [93mpunkt_tab[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('punkt_tab')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtokenizers/punkt_tab/english/[0m

  Searched in:
    - 'C:\\Users\\nanda/nltk_data'
    - 'C:\\Python312\\nltk_data'
    - 'C:\\Python312\\share\\nltk_data'
    - 'C:\\Python312\\lib\\nltk_data'
    - 'C:\\Users\\nanda\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

2024-12-25 21:40:14,607 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:40:14] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:40:14,612 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:40:14] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:41:50,022 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 21:41:51,402 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:42:07,392 - __main__ - INFO - Loading QA model...
2024-12-25 21:42:07,392 - __main__ - INFO - Initializing FastQA...
2024-12-25 21:42:08,667 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:42:08,667 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:42:08,679 - __main__ - INFO - Starting application...
2024-12-25 21:42:08,679 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 21:42:08,679 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 21:42:08,685 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 21:42:08,687 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 21:42:08,701 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:42:08,710 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:42:08,897 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:42:08] "GET / HTTP/1.1" 200 -
2024-12-25 21:42:09,127 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:42:09] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:42:09,127 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:42:09] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:42:16,377 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:42:16] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:42:22,167 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:42:22] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:42:22,170 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:42:22] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:42:36,077 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:42:36] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:42:36,114 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:42:36] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 21:44:51,053 - __main__ - INFO - Loading QA model...
2024-12-25 21:44:51,053 - __main__ - INFO - Initializing FastQA with DistilBERT...
2024-12-25 21:44:52,027 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:44:52,027 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:44:52,037 - __main__ - INFO - Starting application...
2024-12-25 21:44:52,097 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-25 21:44:52,099 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 21:44:52,100 - werkzeug - INFO -  * Restarting with stat
2024-12-25 21:45:06,667 - __main__ - INFO - Loading QA model...
2024-12-25 21:45:06,671 - __main__ - INFO - Initializing FastQA with DistilBERT...
2024-12-25 21:45:08,127 - __main__ - INFO - FastQA initialized successfully
2024-12-25 21:45:08,127 - __main__ - INFO - QA model loaded successfully
2024-12-25 21:45:08,137 - __main__ - INFO - Starting application...
2024-12-25 21:45:08,151 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 21:45:08,159 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 21:45:08,267 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:08] "GET / HTTP/1.1" 200 -
2024-12-25 21:45:08,479 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:08] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 21:45:08,479 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:08] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 21:45:16,557 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:16] "POST /upload HTTP/1.1" 200 -
2024-12-25 21:45:28,680 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:28] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:45:28,680 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:28] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:45:46,121 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:46] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:45:46,127 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:46] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:45:53,372 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:53] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:45:53,442 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 21:45:53] "POST /ask HTTP/1.1" 200 -
2024-12-25 21:55:30,121 - __main__ - INFO - Loading Enhanced QA model...
2024-12-25 21:55:30,121 - __main__ - INFO - Initializing Enhanced QA System...
2024-12-25 22:00:52,178 - __main__ - INFO - Enhanced QA System initialized successfully
2024-12-25 22:00:52,206 - __main__ - INFO - Enhanced QA model loaded successfully
2024-12-25 22:00:52,268 - __main__ - INFO - Starting application...
2024-12-25 22:00:52,270 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 22:00:52,272 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 22:00:52,273 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 22:00:52,275 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 22:00:52,848 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.16.72:5000
2024-12-25 22:00:52,852 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 22:00:52,897 - werkzeug - INFO -  * Restarting with stat
2024-12-25 22:00:59,602 - __main__ - INFO - Loading Enhanced QA model...
2024-12-25 22:00:59,602 - __main__ - INFO - Initializing Enhanced QA System...
2024-12-25 22:01:14,636 - __main__ - INFO - Enhanced QA System initialized successfully
2024-12-25 22:01:14,645 - __main__ - INFO - Enhanced QA model loaded successfully
2024-12-25 22:01:14,666 - __main__ - INFO - Starting application...
2024-12-25 22:01:14,668 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 22:01:14,668 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 22:01:14,669 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 22:01:14,670 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 22:01:14,788 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 22:01:14,817 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 22:01:32,168 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:01:32] "GET / HTTP/1.1" 200 -
2024-12-25 22:01:32,317 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:01:32] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 22:01:32,319 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:01:32] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 22:01:38,997 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:01:38] "POST /upload HTTP/1.1" 200 -
2024-12-25 22:02:05,299 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:02:05] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:02:05,315 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:02:05] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:02:42,777 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:02:42] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:02:42,802 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:02:42] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:03:08,065 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:03:08] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:03:08,149 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:03:08] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:03:49,828 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:03:49] "POST /upload HTTP/1.1" 200 -
2024-12-25 22:03:59,698 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:03:59] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:03:59,705 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:03:59] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:04:11,627 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:04:11] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:04:11,629 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:04:11] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:04:19,921 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:04:19] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:04:19,937 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:04:19] "POST /ask HTTP/1.1" 200 -
2024-12-25 22:04:33,594 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:04:33] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:04:33,619 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:04:33] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:04:44,708 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:04:44] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:04:44,748 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:04:44] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:05:05,864 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:05:05] "POST /upload HTTP/1.1" 200 -
2024-12-25 22:05:08,798 - __main__ - ERROR - Summarization error: index out of range in self
2024-12-25 22:09:19,098 - werkzeug - INFO -  * Detected change in 'C:\\Users\\nanda\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-25 22:09:22,789 - werkzeug - INFO -  * Restarting with stat
2024-12-25 22:09:32,157 - __main__ - INFO - Loading Enhanced QA model...
2024-12-25 22:09:32,158 - __main__ - INFO - Initializing Enhanced QA System...
2024-12-25 22:09:33,182 - __main__ - INFO - Enhanced QA System initialized successfully
2024-12-25 22:09:33,183 - __main__ - INFO - Enhanced QA model loaded successfully
2024-12-25 22:09:33,184 - __main__ - INFO - Starting application...
2024-12-25 22:09:33,185 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 22:09:33,185 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 22:09:33,192 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 22:09:33,192 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 22:09:33,203 - werkzeug - WARNING -  * Debugger is active!
2024-12-25 22:09:33,207 - werkzeug - INFO -  * Debugger PIN: 134-061-009
2024-12-25 22:09:59,045 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:09:59] "GET / HTTP/1.1" 200 -
2024-12-25 22:09:59,207 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:09:59] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-25 22:09:59,207 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:09:59] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-25 22:10:05,436 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:10:05] "POST /upload HTTP/1.1" 200 -
2024-12-25 22:10:10,954 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:10:10] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:10:10,955 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:10:10] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:10:20,007 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:10:20] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:10:20,014 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:10:20] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:10:34,278 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:10:34] "POST /upload HTTP/1.1" 200 -
2024-12-25 22:11:29,886 - werkzeug - INFO - 127.0.0.1 - - [25/Dec/2024 22:11:29] "[35m[1mPOST /ask HTTP/1.1[0m" 503 -
2024-12-25 22:14:57,041 - __main__ - INFO - Loading DocumentQA model...
2024-12-25 22:14:57,041 - __main__ - INFO - Initializing DocumentQA with google/flan-t5-large on cpu...
2024-12-25 22:23:48,155 - __main__ - INFO - Loading RAG Document QA model...
2024-12-25 22:23:48,157 - __main__ - INFO - Initializing RAG Document QA System...
2024-12-25 22:23:48,160 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 22:23:48,160 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-12-25 22:28:53,119 - __main__ - INFO - RAG System initialized successfully
2024-12-25 22:28:53,121 - __main__ - INFO - RAG Document QA model loaded successfully
2024-12-25 22:28:53,127 - __main__ - INFO - Starting application...
2024-12-25 22:28:53,128 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 22:28:53,129 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 22:28:53,130 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 22:28:53,131 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 22:28:53,203 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.16.72:5000
2024-12-25 22:28:53,205 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 22:28:53,211 - werkzeug - INFO -  * Restarting with stat
2024-12-25 22:29:05,499 - __main__ - INFO - Loading RAG Document QA model...
2024-12-25 22:29:05,499 - __main__ - INFO - Initializing RAG Document QA System...
2024-12-25 22:29:05,513 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 22:29:05,513 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-12-25 22:33:59,779 - __main__ - INFO - Loading RAG Document QA model...
2024-12-25 22:33:59,781 - __main__ - INFO - Initializing RAG Document QA System...
2024-12-25 22:33:59,782 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 22:33:59,782 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-12-25 22:34:04,992 - __main__ - INFO - RAG System initialized successfully
2024-12-25 22:34:04,992 - __main__ - INFO - RAG Document QA model loaded successfully
2024-12-25 22:34:04,992 - __main__ - INFO - Starting application...
2024-12-25 22:34:04,994 - __main__ - INFO - Root directory: C:\Users\nanda\Chatbot\chatbot_system
2024-12-25 22:34:04,994 - __main__ - INFO - Template folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\templates
2024-12-25 22:34:04,994 - __main__ - INFO - Static folder: C:\Users\nanda\Chatbot\chatbot_system\frontend\static
2024-12-25 22:34:04,994 - __main__ - INFO - Upload folder: C:\Users\nanda\Chatbot\chatbot_system\temp
2024-12-25 22:34:05,028 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.16.72:5000
2024-12-25 22:34:05,028 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-25 22:34:05,031 - werkzeug - INFO -  * Restarting with stat
2024-12-25 22:34:11,914 - __main__ - INFO - Loading RAG Document QA model...
2024-12-25 22:34:11,914 - __main__ - INFO - Initializing RAG Document QA System...
2024-12-25 22:34:11,923 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-25 22:34:11,923 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-12-26 00:18:50,668 - __main__ - INFO - Starting application...
2024-12-26 00:18:50,669 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:18:50,669 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:18:50,669 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:18:50,696 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 00:18:50,698 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 00:18:50,700 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:18:52,164 - __main__ - INFO - Starting application...
2024-12-26 00:18:52,165 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:18:52,165 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:18:52,166 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:18:52,182 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:18:52,188 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:18:55,736 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:18:55] "GET / HTTP/1.1" 200 -
2024-12-26 00:18:55,900 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:18:55] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 00:18:55,904 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:18:55] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 00:19:01,625 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:19:01] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 00:19:02,420 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:19:02] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 00:19:08,347 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:19:08] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 00:19:14,962 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:19:14] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 00:23:02,388 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:23:02,617 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:23:53,367 - __main__ - INFO - Loading QA system...
2024-12-26 00:23:53,367 - __main__ - INFO - Initializing QA System on cpu...
2024-12-26 00:23:53,370 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-26 00:23:53,370 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-12-26 00:24:12,349 - __main__ - INFO - SentenceTransformer model loaded successfully.
2024-12-26 00:24:12,934 - __main__ - INFO - QA pipeline loaded successfully.
2024-12-26 00:24:12,935 - __main__ - INFO - Creating new FAISS index and initializing chunks list...
2024-12-26 00:24:12,935 - __main__ - INFO - FAISS index and chunks list initialized.
2024-12-26 00:24:12,936 - __main__ - INFO - QA system loaded successfully.
2024-12-26 00:24:12,938 - __main__ - INFO - Starting application...
2024-12-26 00:24:12,938 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:24:12,940 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:24:12,940 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:24:12,964 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.35:5000
2024-12-26 00:24:12,965 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 00:24:12,968 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:24:20,787 - __main__ - INFO - Loading QA system...
2024-12-26 00:24:20,787 - __main__ - INFO - Initializing QA System on cpu...
2024-12-26 00:24:20,790 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2024-12-26 00:24:20,790 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-12-26 00:24:24,602 - __main__ - INFO - SentenceTransformer model loaded successfully.
2024-12-26 00:24:25,173 - __main__ - INFO - QA pipeline loaded successfully.
2024-12-26 00:24:25,173 - __main__ - INFO - Creating new FAISS index and initializing chunks list...
2024-12-26 00:24:25,174 - __main__ - INFO - FAISS index and chunks list initialized.
2024-12-26 00:24:25,174 - __main__ - INFO - QA system loaded successfully.
2024-12-26 00:24:25,176 - __main__ - INFO - Starting application...
2024-12-26 00:24:25,176 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:24:25,176 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:24:25,178 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:24:25,186 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:24:25,191 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:24:25,214 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:24:25] "GET / HTTP/1.1" 200 -
2024-12-26 00:24:34,646 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /upload
2024-12-26 00:24:34,647 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:24:34] "[31m[1mPOST /upload HTTP/1.1[0m" 403 -
2024-12-26 00:24:38,162 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /upload
2024-12-26 00:24:38,163 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:24:38] "[31m[1mPOST /upload HTTP/1.1[0m" 403 -
2024-12-26 00:24:52,642 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:24:52] "GET / HTTP/1.1" 200 -
2024-12-26 00:24:52,729 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:24:52] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:24:52,746 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:24:52] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 00:24:58,160 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:24:58] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 00:33:27,418 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:33:28,389 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:33:29,074 - __main__ - INFO - Starting application...
2024-12-26 00:33:29,075 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:33:29,075 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:33:29,075 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:33:29,097 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:33:29,104 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:33:37,702 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:33:37] "GET / HTTP/1.1" 200 -
2024-12-26 00:33:37,834 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:33:37] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:33:37,836 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:33:37] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 00:33:41,847 - __main__ - INFO - File saved successfully: 875c389be7b34c4962d2b1e1d88510502cb3aa054a5a9595655510fbb43f5524.pdf
2024-12-26 00:33:41,876 - __main__ - INFO - Temporary file removed: 875c389be7b34c4962d2b1e1d88510502cb3aa054a5a9595655510fbb43f5524.pdf
2024-12-26 00:33:41,877 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:33:41] "POST /upload HTTP/1.1" 200 -
2024-12-26 00:33:59,212 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:33:59] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:34:04,671 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:34:04] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:34:06,403 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:34:06] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:34:06,603 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:34:06] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:34:17,660 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:34:17] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:34:19,146 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:34:19] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:34:19,638 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:34:19] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:34:19,840 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:34:19] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:35:08,536 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:35:08,633 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:35:10,382 - __main__ - INFO - Starting application...
2024-12-26 00:35:10,382 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:35:10,382 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:35:10,382 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:35:10,404 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:35:10,412 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:35:46,599 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:35:46,801 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:35:48,579 - __main__ - INFO - Starting application...
2024-12-26 00:35:48,580 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:35:48,580 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:35:48,581 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:35:48,600 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:35:48,607 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:35:58,086 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:35:58] "GET / HTTP/1.1" 200 -
2024-12-26 00:35:58,206 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:35:58] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:35:58,211 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:35:58] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 00:36:07,627 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:36:07] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 00:36:36,847 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:36:37,027 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:36:37,698 - __main__ - INFO - Starting application...
2024-12-26 00:36:37,698 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:36:37,698 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:36:37,700 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:36:37,722 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:36:37,729 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:36:43,465 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:36:43] "GET / HTTP/1.1" 200 -
2024-12-26 00:36:43,552 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:36:43] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:36:43,567 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:36:43] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 00:36:58,177 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:36:58] "GET / HTTP/1.1" 200 -
2024-12-26 00:36:58,195 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:36:58] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:36:58,197 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:36:58] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 00:37:01,548 - __main__ - INFO - File saved successfully: 2f854a294350146e0ee0f22fecf69690f62a8f1f549b3772d02cc0fed37f9c18.pdf
2024-12-26 00:37:01,575 - __main__ - INFO - Temporary file removed: 2f854a294350146e0ee0f22fecf69690f62a8f1f549b3772d02cc0fed37f9c18.pdf
2024-12-26 00:37:01,577 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:37:01] "POST /upload HTTP/1.1" 200 -
2024-12-26 00:37:13,181 - __main__ - INFO - File saved successfully: 291c2198c27e2a463f236f680bd7e6aa15d9757709bfc2c57dd29d95a452cb1f.pdf
2024-12-26 00:37:13,205 - __main__ - INFO - Temporary file removed: 291c2198c27e2a463f236f680bd7e6aa15d9757709bfc2c57dd29d95a452cb1f.pdf
2024-12-26 00:37:13,209 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:37:13] "POST /upload HTTP/1.1" 200 -
2024-12-26 00:37:31,470 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:37:31] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 00:42:37,401 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:42:37,478 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:42:39,101 - __main__ - INFO - Starting application...
2024-12-26 00:42:39,102 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:42:39,102 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:42:39,102 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:42:39,125 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:42:39,133 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:42:45,774 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:42:45] "GET / HTTP/1.1" 200 -
2024-12-26 00:42:45,882 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:42:45] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:42:45,885 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:42:45] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 00:42:51,473 - __main__ - INFO - File saved successfully: 258ba48a00d4f24cd7784d860199f43bc3571d5f64308b16942535485a8ab1dc.pdf
2024-12-26 00:42:51,509 - __main__ - INFO - Temporary file removed: 258ba48a00d4f24cd7784d860199f43bc3571d5f64308b16942535485a8ab1dc.pdf
2024-12-26 00:42:51,509 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:42:51] "POST /upload HTTP/1.1" 200 -
2024-12-26 00:42:57,575 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:42:57,575 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:42:57,575 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:42:57] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:42:57,575 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:42:57] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:42:59,971 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:42:59,971 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:42:59,973 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:42:59] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:42:59,974 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:42:59] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:00,875 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:00,876 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:00,877 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:00] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:00,878 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:00] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:01,168 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:01,170 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:01,170 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:01] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:01,171 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:01] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:01,421 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:01,422 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:01,423 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:01] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:01,424 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:01] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:01,627 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:01,627 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:01,629 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:01] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:01,631 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:01] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:43:07,160 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:07] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:43:14,747 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:43:14,750 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:43:14] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:45:49,135 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:45:49,333 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:45:51,071 - __main__ - INFO - Starting application...
2024-12-26 00:45:51,071 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:45:51,071 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:45:51,071 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:45:51,094 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:45:51,101 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:45:58,641 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:45:58] "GET / HTTP/1.1" 200 -
2024-12-26 00:45:58,759 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:45:58] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 00:45:58,764 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:45:58] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:46:03,486 - __main__ - INFO - File saved successfully: 86912b6f4857aa8e5e132291552b5a85a0987c2534e2c7b14b2c92004bef44a2.pdf
2024-12-26 00:46:03,509 - __main__ - INFO - Temporary file removed: 86912b6f4857aa8e5e132291552b5a85a0987c2534e2c7b14b2c92004bef44a2.pdf
2024-12-26 00:46:03,510 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:46:03] "POST /upload HTTP/1.1" 200 -
2024-12-26 00:46:10,731 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:46:10,735 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:46:10] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:46:17,084 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:46:17,086 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:46:17] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:46:34,958 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:46:34] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:46:39,816 - __main__ - ERROR - Error querying OpenAI GPT-3: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

2024-12-26 00:46:39,818 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:46:39] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:47:52,099 - __main__ - INFO - Starting application...
2024-12-26 00:47:52,099 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:47:52,099 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:47:52,099 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:47:52,134 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 00:47:52,134 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 00:47:52,136 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:47:53,716 - __main__ - INFO - Starting application...
2024-12-26 00:47:53,716 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:47:53,716 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:47:53,720 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:47:53,736 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:47:53,743 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:48:01,855 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:48:01] "GET / HTTP/1.1" 200 -
2024-12-26 00:48:01,981 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:48:01] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:48:01,982 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:48:01] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 00:48:06,926 - __main__ - INFO - File saved successfully: cf63aef68a2273df522ea85db16b537da72ad819156bcad3770749f42e752da6.pdf
2024-12-26 00:48:06,963 - __main__ - INFO - Temporary file removed: cf63aef68a2273df522ea85db16b537da72ad819156bcad3770749f42e752da6.pdf
2024-12-26 00:48:06,964 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:48:06] "POST /upload HTTP/1.1" 200 -
2024-12-26 00:48:15,403 - openai - INFO - error_code=model_not_found error_message='The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
2024-12-26 00:48:15,404 - __main__ - ERROR - Error querying OpenAI GPT-3: The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations
2024-12-26 00:48:15,405 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:48:15] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:50:53,591 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:50:53,865 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:50:55,644 - __main__ - INFO - Starting application...
2024-12-26 00:50:55,645 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:50:55,645 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:50:55,646 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:50:55,666 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:50:55,673 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:50:59,027 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:50:59] "GET / HTTP/1.1" 200 -
2024-12-26 00:50:59,133 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:50:59] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:50:59,141 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:50:59] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 00:51:02,686 - __main__ - INFO - File saved successfully: 754a44dfbcccdeb2fed37bc0beac97fd2d5600db1943752c8ba3d5133c9cda6a.pdf
2024-12-26 00:51:02,716 - __main__ - INFO - Temporary file removed: 754a44dfbcccdeb2fed37bc0beac97fd2d5600db1943752c8ba3d5133c9cda6a.pdf
2024-12-26 00:51:02,717 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:51:02] "POST /upload HTTP/1.1" 200 -
2024-12-26 00:51:09,992 - openai - INFO - error_code=insufficient_quota error_message='You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.' error_param=None error_type=insufficient_quota message='OpenAI API error received' stream_error=False
2024-12-26 00:51:09,998 - __main__ - ERROR - OpenAI API error: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.
2024-12-26 00:51:09,998 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:51:09] "[35m[1mPOST /ask HTTP/1.1[0m" 500 -
2024-12-26 00:58:52,927 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 00:58:53,080 - werkzeug - INFO -  * Restarting with stat
2024-12-26 00:59:33,319 - __main__ - INFO - Starting application...
2024-12-26 00:59:33,319 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 00:59:33,319 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 00:59:33,321 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 00:59:33,330 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 00:59:33,336 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 00:59:33,370 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:59:33] "GET / HTTP/1.1" 200 -
2024-12-26 00:59:33,476 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:59:33] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 00:59:33,496 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 00:59:33] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:00:00,674 - __main__ - INFO - File saved successfully: c8ba09f0185e4bc6345c6da2dd2d513c8ce36b648e5e3a1090f61a5ef8297b33.pdf
2024-12-26 01:00:00,705 - __main__ - INFO - Temporary file removed: c8ba09f0185e4bc6345c6da2dd2d513c8ce36b648e5e3a1090f61a5ef8297b33.pdf
2024-12-26 01:00:00,707 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:00] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:00:10,389 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:00:10,390 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:00:32,277 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:32] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:00:32,284 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:32] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:00:48,869 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:48] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:00:48,871 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:48] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:00:55,749 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:55] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:00:55,749 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:00:55] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:05,316 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:05] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:05,324 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:05] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:24,335 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:24] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:24,338 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:24] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:30,916 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:30] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:30,920 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:30] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:41,230 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:41] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:41,231 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:41] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:01:44,036 - __main__ - INFO - File saved successfully: d318acc9d75de68f0f33dd65485552883faabeaeab7925c7a0d723720ff6f3ce.pdf
2024-12-26 01:01:44,061 - __main__ - INFO - Temporary file removed: d318acc9d75de68f0f33dd65485552883faabeaeab7925c7a0d723720ff6f3ce.pdf
2024-12-26 01:01:44,062 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:44] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:01:55,129 - __main__ - INFO - File saved successfully: d43c0b6741e225f93f9146bd8d377dab68b6c2e5eee796217fd5644f92305a14.pdf
2024-12-26 01:01:55,458 - __main__ - INFO - Temporary file removed: d43c0b6741e225f93f9146bd8d377dab68b6c2e5eee796217fd5644f92305a14.pdf
2024-12-26 01:01:55,458 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:01:55] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:02:08,540 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:08] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:08,557 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:08] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:20,837 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:20] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:20,839 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:20] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:29,920 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:29] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:29,927 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:29] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:40,055 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:40] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:40,076 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:40] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:53,610 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:53] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:02:53,621 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:02:53] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:05:33,179 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 01:05:34,052 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:10:32,782 - __main__ - INFO - Starting application...
2024-12-26 01:10:32,783 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:10:32,783 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:10:32,783 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:10:32,802 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 01:10:32,803 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 01:10:32,805 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:10:42,736 - __main__ - INFO - Starting application...
2024-12-26 01:10:42,737 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:10:42,737 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:10:42,738 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:10:42,749 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 01:10:42,755 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 01:11:05,158 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:05] "GET / HTTP/1.1" 200 -
2024-12-26 01:11:05,306 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:05] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:11:05,306 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:05] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:11:16,148 - __main__ - INFO - File saved successfully: 058207ebed06570143067dc28d4ef988fca8269f705f45c59c7ad33ac074e459.pdf
2024-12-26 01:11:16,171 - __main__ - INFO - Temporary file removed: 058207ebed06570143067dc28d4ef988fca8269f705f45c59c7ad33ac074e459.pdf
2024-12-26 01:11:16,172 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:16] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:11:30,030 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:30] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:11:30,056 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:30] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:11:42,743 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:42] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:11:42,745 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:42] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:11:54,141 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:54] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:11:54,142 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:11:54] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:12:12,286 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:12:12] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:12:12,288 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:12:12] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:12:43,673 - __main__ - INFO - File saved successfully: 5f1b1daa2d575cda1100a76b1b60f52786007d2ca24fc3cc6caf7ba2c05d1b9f.pdf
2024-12-26 01:12:44,083 - __main__ - INFO - Temporary file removed: 5f1b1daa2d575cda1100a76b1b60f52786007d2ca24fc3cc6caf7ba2c05d1b9f.pdf
2024-12-26 01:12:44,084 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:12:44] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:13:00,683 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:13:00] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:13:00,683 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:13:00] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:13:20,241 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:13:20] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:13:20,273 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:13:20] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:13:30,823 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:13:30] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:13:30,851 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:13:30] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:13:39,273 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:13:39] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:13:39,273 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:13:39] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:16:22,640 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 01:16:23,675 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:16:32,044 - __main__ - INFO - Starting application...
2024-12-26 01:16:32,045 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:16:32,045 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:16:32,045 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:16:32,054 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 01:16:32,059 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 01:16:32,084 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:16:32] "GET / HTTP/1.1" 200 -
2024-12-26 01:16:32,210 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:16:32] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:16:32,214 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:16:32] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:16:41,057 - __main__ - INFO - File saved successfully: 8627edfcfe1ba0de58d8966ed3c21e4cfbd7dfa6af0b0a5b90e92463e3f88a7a.pdf
2024-12-26 01:16:41,085 - __main__ - INFO - Temporary file removed: 8627edfcfe1ba0de58d8966ed3c21e4cfbd7dfa6af0b0a5b90e92463e3f88a7a.pdf
2024-12-26 01:16:41,087 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:16:41] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:16:47,327 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:16:47] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:16:47,328 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:16:47] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:16:54,911 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:16:54] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:16:54,927 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:16:54] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:17:01,923 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:01] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:17:01,942 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:01] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:17:07,207 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:07] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:17:07,226 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:07] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:17:40,747 - __main__ - INFO - File saved successfully: 6545c4489b19dfd9bf070c76bb58f7f1f68999d1d655eb9930b5ccc2545b26b0.pdf
2024-12-26 01:17:41,089 - __main__ - INFO - Temporary file removed: 6545c4489b19dfd9bf070c76bb58f7f1f68999d1d655eb9930b5ccc2545b26b0.pdf
2024-12-26 01:17:41,090 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:41] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:17:49,134 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:49] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:17:49,143 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:49] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:17:56,228 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:56] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:17:56,229 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:17:56] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:18:03,278 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:18:03] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:18:03,299 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:18:03] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:18:16,026 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:18:16] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:18:16,037 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:18:16] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:18:23,555 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:18:23] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:18:23,566 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:18:23] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:18:40,468 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:18:40] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:18:40,476 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:18:40] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:19:09,292 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:19:09] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:19:09,297 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:19:09] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:22:16,848 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 01:22:17,714 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:22:25,976 - __main__ - INFO - Starting application...
2024-12-26 01:22:25,977 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:22:25,977 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:22:25,977 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:22:25,984 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 01:22:25,988 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 01:22:37,221 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:22:37] "GET / HTTP/1.1" 200 -
2024-12-26 01:22:37,355 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:22:37] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:22:37,358 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:22:37] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:22:42,096 - __main__ - INFO - File saved successfully: b377ed961264170e7c75f7d0ab64882300ac3900257a7fdfcfc38724141a812d.pdf
2024-12-26 01:22:42,417 - __main__ - INFO - Temporary file removed: b377ed961264170e7c75f7d0ab64882300ac3900257a7fdfcfc38724141a812d.pdf
2024-12-26 01:22:42,418 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:22:42] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:22:58,771 - __main__ - ERROR - Error querying Hugging Face API: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:22:58,774 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:22:58] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:23:08,532 - __main__ - ERROR - Error querying Hugging Face API: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:23:08,533 - __main__ - ERROR - Error querying Hugging Face API: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:23:08,535 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:23:08] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:23:08,536 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:23:08] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:23:19,158 - __main__ - ERROR - Error querying Hugging Face API: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:23:19,161 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:23:19] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:23:19,188 - __main__ - ERROR - Error querying Hugging Face API: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:23:19,189 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:23:19] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:28:57,380 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 01:28:58,337 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:29:07,210 - __main__ - INFO - Starting application...
2024-12-26 01:29:07,210 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:29:07,210 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:29:07,210 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:29:07,216 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 01:29:07,224 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 01:29:08,011 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 01:29:08,013 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:08] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 01:29:09,117 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 01:29:09,118 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:09] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 01:29:09,949 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 01:29:09,950 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:09] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 01:29:12,848 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:12] "GET / HTTP/1.1" 200 -
2024-12-26 01:29:12,979 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:12] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:29:12,981 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:12] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:29:22,464 - __main__ - INFO - File saved successfully: 94a041c7af010108a90ddbcad1bf317e7245a0449c2c7d6dce3e07960d3387c5.pdf
2024-12-26 01:29:22,492 - __main__ - INFO - Temporary file removed: 94a041c7af010108a90ddbcad1bf317e7245a0449c2c7d6dce3e07960d3387c5.pdf
2024-12-26 01:29:22,493 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:22] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:29:26,744 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:26] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:29:26,744 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:26] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:29:38,725 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:38] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:29:53,540 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:29:53] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:30:05,957 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:30:05] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:30:05,958 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:30:05] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:30:17,741 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:30:17] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:30:32,740 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:30:32] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:30:52,275 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:30:52] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:30:52,279 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:30:52] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:31:16,336 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:31:16] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:31:21,536 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:31:21] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:31:25,715 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:31:25] "GET / HTTP/1.1" 200 -
2024-12-26 01:31:25,736 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:31:25] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:31:25,745 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:31:25] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:31:42,754 - __main__ - INFO - File saved successfully: 691eb2989e0dbb7bef93b9811b712f173ddd494489e072551c6d4d1d25a86511.pdf
2024-12-26 01:31:43,059 - __main__ - INFO - Temporary file removed: 691eb2989e0dbb7bef93b9811b712f173ddd494489e072551c6d4d1d25a86511.pdf
2024-12-26 01:31:43,060 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:31:43] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:32:03,011 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:32:03] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:32:10,234 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:32:10] "GET / HTTP/1.1" 200 -
2024-12-26 01:32:10,252 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:32:10] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:32:10,258 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:32:10] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:34:37,784 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 01:34:38,778 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:34:46,748 - __main__ - INFO - Starting application...
2024-12-26 01:34:46,750 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:34:46,750 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:34:46,751 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:34:46,761 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 01:34:46,768 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 01:35:02,088 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:35:02] "GET / HTTP/1.1" 200 -
2024-12-26 01:35:02,216 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:35:02] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:35:02,217 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:35:02] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:35:08,848 - __main__ - INFO - File saved successfully: 470533ef9459e1f290004841076180c0d36f0383db79adde9811e97b9d42e091.pdf
2024-12-26 01:35:09,149 - __main__ - INFO - Temporary file removed: 470533ef9459e1f290004841076180c0d36f0383db79adde9811e97b9d42e091.pdf
2024-12-26 01:35:09,151 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:35:09] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:35:24,311 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:35:24] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:35:39,413 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:35:39] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:35:56,567 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:35:56] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:35:56,584 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:35:56] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:36:02,824 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:36:02] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:39:03,991 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 01:39:04,851 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:39:13,338 - __main__ - INFO - Starting application...
2024-12-26 01:39:13,338 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:39:13,339 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:39:13,339 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:39:13,347 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 01:39:13,354 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 01:39:23,455 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 01:39:24,493 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:39:36,832 - __main__ - INFO - Starting application...
2024-12-26 01:39:36,832 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:39:36,840 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:39:36,840 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:39:36,850 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 01:39:36,860 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 01:39:36,898 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:39:36] "GET / HTTP/1.1" 200 -
2024-12-26 01:39:37,061 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:39:37] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:39:37,075 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:39:37] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:39:43,115 - __main__ - INFO - File saved successfully: 16f224610d20653cc52c4e5d2114fed88d5558f7afe110c796fe6695388c33c5.pdf
2024-12-26 01:39:43,559 - __main__ - INFO - Temporary file removed: 16f224610d20653cc52c4e5d2114fed88d5558f7afe110c796fe6695388c33c5.pdf
2024-12-26 01:39:43,560 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:39:43] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:39:58,142 - __main__ - ERROR - Error querying LLM API: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:39:58,143 - __main__ - ERROR - Error querying LLM API: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:39:58,145 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:39:58] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:39:58,145 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:39:58] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:40:16,560 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:40:16] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:40:16,580 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:40:16] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:40:37,220 - __main__ - ERROR - Error querying LLM API: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:40:37,220 - __main__ - ERROR - Error querying LLM API: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/gpt2
2024-12-26 01:40:37,220 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:40:37] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:40:37,223 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:40:37] "[33mPOST /ask HTTP/1.1[0m" 404 -
2024-12-26 01:40:40,599 - __main__ - INFO - File saved successfully: 6aeb6d3887c36179f0bc615bc4f31327c8a75165cd0bc89843682d44f68cc63c.pdf
2024-12-26 01:40:41,059 - __main__ - INFO - Temporary file removed: 6aeb6d3887c36179f0bc615bc4f31327c8a75165cd0bc89843682d44f68cc63c.pdf
2024-12-26 01:40:41,062 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:40:41] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:40:57,342 - __main__ - INFO - File saved successfully: c79a640b93d57c4f912e7c857ce7baa2c432a2d8fc7c513c16b43183e30ed6b4.pdf
2024-12-26 01:40:57,376 - __main__ - INFO - Temporary file removed: c79a640b93d57c4f912e7c857ce7baa2c432a2d8fc7c513c16b43183e30ed6b4.pdf
2024-12-26 01:40:57,377 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:40:57] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:41:04,014 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:41:04] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:41:04,033 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:41:04] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:41:17,388 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:41:17] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:41:29,808 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:41:29] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:42:05,401 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:42:05] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:42:05,421 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:42:05] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:42:23,715 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:42:23] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:42:23,728 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:42:23] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:42:41,827 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:42:41] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:42:57,696 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:42:57] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:45:36,297 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 01:45:37,299 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:54:55,234 - __main__ - INFO - Successfully loaded QA model
2024-12-26 01:54:55,238 - __main__ - INFO - Starting application...
2024-12-26 01:54:55,239 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:54:55,239 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:54:55,239 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:54:55,265 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 01:54:55,265 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 01:54:55,269 - werkzeug - INFO -  * Restarting with stat
2024-12-26 01:55:07,887 - __main__ - INFO - Successfully loaded QA model
2024-12-26 01:55:07,895 - __main__ - INFO - Starting application...
2024-12-26 01:55:07,895 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 01:55:07,898 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 01:55:07,899 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 01:55:07,920 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 01:55:07,931 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 01:55:07,970 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:07] "GET / HTTP/1.1" 200 -
2024-12-26 01:55:08,095 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:08] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:55:08,125 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:08] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 01:55:13,358 - __main__ - INFO - File saved successfully: 5055071e97b113284b803970340db4208a2e7de540b00df83a1a69e3a0d3da48.pdf
2024-12-26 01:55:13,390 - __main__ - INFO - Temporary file removed: 5055071e97b113284b803970340db4208a2e7de540b00df83a1a69e3a0d3da48.pdf
2024-12-26 01:55:13,395 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:13] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:55:17,477 - __main__ - WARNING - Malicious input detected in question: who is elara?
2024-12-26 01:55:17,477 - __main__ - WARNING - Malicious input detected in question: who is elara?
2024-12-26 01:55:17,481 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:17] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 01:55:17,481 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:17] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 01:55:27,967 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:27] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:55:27,967 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:27] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:55:35,474 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:35] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:55:35,479 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:35] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:55:40,482 - __main__ - WARNING - Malicious input detected in question: what did elara discover?
2024-12-26 01:55:40,484 - __main__ - WARNING - Malicious input detected in question: what did elara discover?
2024-12-26 01:55:40,484 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:40] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 01:55:40,485 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:40] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 01:55:47,119 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:47] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:55:47,134 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:55:47] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:56:17,262 - __main__ - INFO - File saved successfully: ecb2b16c8d3a39d52157a55eebdba99641ee3af2a8fc731e5962977f4dc13a3b.pdf
2024-12-26 01:56:17,739 - __main__ - INFO - Temporary file removed: ecb2b16c8d3a39d52157a55eebdba99641ee3af2a8fc731e5962977f4dc13a3b.pdf
2024-12-26 01:56:17,740 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:56:17] "POST /upload HTTP/1.1" 200 -
2024-12-26 01:56:25,955 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:56:25] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:56:26,000 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:56:26] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:56:48,710 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:56:48] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:56:49,304 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:56:49] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:57:21,952 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:57:21] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:57:23,632 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:57:23] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:57:24,182 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:57:24] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:57:24,200 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:57:24] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:57:31,843 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:57:31] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:57:32,134 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:57:32] "POST /ask HTTP/1.1" 200 -
2024-12-26 01:59:41,702 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:59:41] "GET / HTTP/1.1" 200 -
2024-12-26 01:59:41,836 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:59:41] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 01:59:41,839 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 01:59:41] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 02:00:02,075 - __main__ - INFO - File saved successfully: 83d157f99be21c8a6ab5dd627921f75560e0c141162794d8e7c97af38e405dbd.pdf
2024-12-26 02:00:05,886 - __main__ - INFO - Temporary file removed: 83d157f99be21c8a6ab5dd627921f75560e0c141162794d8e7c97af38e405dbd.pdf
2024-12-26 02:00:05,892 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:00:05] "POST /upload HTTP/1.1" 200 -
2024-12-26 02:01:17,177 - __main__ - WARNING - Malicious input detected in question: select * from users?
2024-12-26 02:01:17,189 - __main__ - WARNING - Malicious input detected in question: select * from users?
2024-12-26 02:01:17,192 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:01:17] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:01:17,198 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:01:17] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:01:21,152 - __main__ - WARNING - Malicious input detected in question: select * from users
2024-12-26 02:01:21,170 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:01:21] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:01:21,172 - __main__ - WARNING - Malicious input detected in question: select * from users
2024-12-26 02:01:21,180 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:01:21] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:01:50,983 - __main__ - WARNING - Malicious input detected in question: what are the languages known?
2024-12-26 02:01:50,999 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:01:50] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:01:51,000 - __main__ - WARNING - Malicious input detected in question: what are the languages known?
2024-12-26 02:01:51,008 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:01:51] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:02:07,360 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:02:07] "POST /ask HTTP/1.1" 200 -
2024-12-26 02:02:07,405 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:02:07] "POST /ask HTTP/1.1" 200 -
2024-12-26 02:02:27,719 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 02:02:34,758 - werkzeug - INFO -  * Restarting with stat
2024-12-26 02:03:48,626 - __main__ - INFO - Successfully loaded QA model
2024-12-26 02:03:48,642 - __main__ - INFO - Starting application...
2024-12-26 02:03:48,646 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 02:03:48,649 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 02:03:48,653 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 02:03:48,729 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 02:03:48,774 - werkzeug - INFO -  * Debugger PIN: 124-268-802
2024-12-26 02:03:48,915 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 02:03:48,918 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 02:03:48,931 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:03:48] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 02:03:48,941 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:03:48] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 02:07:39,452 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:07:39] "GET / HTTP/1.1" 200 -
2024-12-26 02:07:40,412 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:07:40] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 02:07:40,428 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:07:40] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 02:09:14,883 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:14] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 02:09:18,604 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:18] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 02:09:19,530 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:19] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 02:09:19,718 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:19] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 02:09:24,703 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:24] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 02:09:25,333 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:25] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 02:09:25,576 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:25] "[31m[1mPOST /upload HTTP/1.1[0m" 400 -
2024-12-26 02:09:57,057 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:57] "GET / HTTP/1.1" 200 -
2024-12-26 02:09:57,071 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:57] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 02:09:57,074 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:09:57] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 02:10:00,610 - __main__ - INFO - File saved successfully: 51e7344d3e0c4ca32e192b4661d6bf3a8c1515745bc821a556d8fbe5ebdea73f.pdf
2024-12-26 02:10:00,623 - __main__ - INFO - Temporary file removed: 51e7344d3e0c4ca32e192b4661d6bf3a8c1515745bc821a556d8fbe5ebdea73f.pdf
2024-12-26 02:10:00,623 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:00] "POST /upload HTTP/1.1" 200 -
2024-12-26 02:10:16,404 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:16] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:10:16,405 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:16] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:10:19,252 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:19] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:10:19,253 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:19] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:10:34,397 - __main__ - INFO - File saved successfully: 8c5f85ce5666ebcf6510837bc3accc57ba18c390f8318b860f4b144740f29f69.pdf
2024-12-26 02:10:34,581 - __main__ - INFO - Temporary file removed: 8c5f85ce5666ebcf6510837bc3accc57ba18c390f8318b860f4b144740f29f69.pdf
2024-12-26 02:10:34,583 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:34] "POST /upload HTTP/1.1" 200 -
2024-12-26 02:10:48,788 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:48] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:10:51,801 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:51] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:10:52,790 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:52] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:10:53,020 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:10:53] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 02:11:20,498 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:11:20] "GET / HTTP/1.1" 200 -
2024-12-26 02:11:20,508 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:11:20] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 02:11:20,510 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:11:20] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 02:11:33,673 - __main__ - INFO - File saved successfully: e706fd409952fa412d6a0ca20d5e682e6546110adef019bfd6b5fdeef2b82256.pdf
2024-12-26 02:11:34,010 - __main__ - INFO - Temporary file removed: e706fd409952fa412d6a0ca20d5e682e6546110adef019bfd6b5fdeef2b82256.pdf
2024-12-26 02:11:34,010 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:11:34] "POST /upload HTTP/1.1" 200 -
2024-12-26 02:11:57,012 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:11:57] "POST /ask HTTP/1.1" 200 -
2024-12-26 02:12:07,808 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:12:07] "POST /ask HTTP/1.1" 200 -
2024-12-26 02:12:14,992 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 02:12:14] "POST /ask HTTP/1.1" 200 -
2024-12-26 07:52:28,937 - __main__ - INFO - Successfully loaded QA model
2024-12-26 07:59:26,145 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:02:15,735 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:02:15,738 - __main__ - INFO - Starting application...
2024-12-26 08:02:15,738 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:02:15,738 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:02:15,738 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:02:15,758 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 08:02:15,758 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 08:02:15,761 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:02:17,165 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:02:18,986 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:02:20,786 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:02:22,254 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:02:22,256 - __main__ - INFO - Starting application...
2024-12-26 08:02:22,256 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:02:22,256 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:02:22,256 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:02:22,261 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:02:22,265 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:02:26,585 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:02:27,562 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:17:09,447 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:17:11,671 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:17:16,466 - __main__ - ERROR - Error loading RAG model: The repository for wiki_dpr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wiki_dpr.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2024-12-26 08:17:16,474 - __main__ - INFO - Starting application...
2024-12-26 08:17:16,475 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:17:16,475 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:17:16,475 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:17:16,485 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 08:17:16,486 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 08:17:16,487 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:17:17,037 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:17:18,435 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:17:23,369 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:17:23,511 - __mp_main__ - ERROR - Error loading RAG model: The repository for wiki_dpr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wiki_dpr.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2024-12-26 08:17:26,298 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:17:30,927 - __main__ - ERROR - Error loading RAG model: The repository for wiki_dpr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wiki_dpr.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2024-12-26 08:17:30,936 - __main__ - INFO - Starting application...
2024-12-26 08:17:30,936 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:17:30,936 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:17:30,936 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:17:30,941 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:17:30,944 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:17:31,383 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:17:33,571 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:17:37,590 - __mp_main__ - ERROR - Error loading RAG model: The repository for wiki_dpr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wiki_dpr.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
2024-12-26 08:26:50,737 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:26:53,265 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:26:53,267 - __main__ - INFO - Starting application...
2024-12-26 08:26:53,267 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:26:53,267 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:26:53,267 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:26:53,276 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 08:26:53,276 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 08:26:53,278 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:26:57,344 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:26:58,305 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:26:59,089 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:27:01,351 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:27:01,353 - __main__ - INFO - Starting application...
2024-12-26 08:27:01,353 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:27:01,353 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:27:01,354 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:27:01,358 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:27:01,361 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:27:05,244 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:27:06,233 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:32:31,181 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:32:32,856 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:32:32,858 - __main__ - INFO - Starting application...
2024-12-26 08:32:32,858 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:32:32,858 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:32:32,858 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:32:32,873 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 08:32:32,873 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 08:32:32,874 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:32:37,245 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:32:38,229 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:32:38,352 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:32:40,317 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:32:40,319 - __main__ - INFO - Starting application...
2024-12-26 08:32:40,320 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:32:40,320 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:32:40,320 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:32:40,326 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:32:40,330 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:32:40,348 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:32:40] "GET / HTTP/1.1" 200 -
2024-12-26 08:32:40,461 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:32:40] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 08:32:40,464 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:32:40] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 08:32:40,519 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:32:40] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2024-12-26 08:32:44,384 - __main__ - INFO - File saved successfully: 9f6a949a5f2529d4dcbd12ea03214ca74c1918369c5ee69fe45d1e4afc1a8f99
2024-12-26 08:32:44,411 - __main__ - INFO - Temporary file removed: 9f6a949a5f2529d4dcbd12ea03214ca74c1918369c5ee69fe45d1e4afc1a8f99
2024-12-26 08:32:44,411 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:32:44] "POST /upload HTTP/1.1" 200 -
2024-12-26 08:32:45,303 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:32:46,768 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:32:55,497 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:32:55] "POST /summarize HTTP/1.1" 200 -
2024-12-26 08:33:29,919 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:33:29] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:33:43,169 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:33:43] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:33:47,079 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:33:47,079 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:33:47] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:33:47,082 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:33:47,082 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:33:47] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:34:01,863 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:34:01] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:34:01,865 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:34:01] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:34:12,468 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:34:12] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:34:12,476 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:34:12] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:35:18,557 - __main__ - INFO - File saved successfully: 9aed9200426ae7e1cba349afc05d7bf424a370f72027381f47567059939c0d0b
2024-12-26 08:35:18,885 - __main__ - INFO - Temporary file removed: 9aed9200426ae7e1cba349afc05d7bf424a370f72027381f47567059939c0d0b
2024-12-26 08:35:18,885 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:35:18] "POST /upload HTTP/1.1" 200 -
2024-12-26 08:35:28,342 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:35:28] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:35:28,344 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:35:28] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:35:39,992 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:35:39,994 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:35:39] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:35:40,039 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:35:40,040 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:35:40] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:38:50,002 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 08:38:51,173 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:39:00,181 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:39:03,303 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:39:03,308 - __main__ - INFO - Starting application...
2024-12-26 08:39:03,309 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:39:03,309 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:39:03,310 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:39:03,318 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:39:03,324 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:39:04,296 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:04] "GET / HTTP/1.1" 200 -
2024-12-26 08:39:04,533 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:04] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 08:39:04,536 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:04] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 08:39:08,997 - __main__ - INFO - File saved successfully: ee51ed760ff7f894da378e5cfeaa24a81697de10d64e7d47eaf1115dd151cf01
2024-12-26 08:39:09,394 - __main__ - INFO - Temporary file removed: ee51ed760ff7f894da378e5cfeaa24a81697de10d64e7d47eaf1115dd151cf01
2024-12-26 08:39:09,395 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:09] "POST /upload HTTP/1.1" 200 -
2024-12-26 08:39:10,928 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:39:11,758 - __main__ - ERROR - Summarization error: index out of range in self
2024-12-26 08:39:11,762 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:11] "[35m[1mPOST /summarize HTTP/1.1[0m" 500 -
2024-12-26 08:39:11,923 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:39:22,858 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:22] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:39:33,455 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:33] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:39:41,999 - __main__ - INFO - File saved successfully: eb9d405514bfe12067ed456dce9d21a80664f96f907946afb0505d6180403329
2024-12-26 08:39:42,050 - __main__ - INFO - Temporary file removed: eb9d405514bfe12067ed456dce9d21a80664f96f907946afb0505d6180403329
2024-12-26 08:39:42,051 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:42] "POST /upload HTTP/1.1" 200 -
2024-12-26 08:39:51,834 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:39:51] "POST /summarize HTTP/1.1" 200 -
2024-12-26 08:44:13,150 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:44:13] "GET / HTTP/1.1" 200 -
2024-12-26 08:44:13,245 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:44:13] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 08:44:13,245 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:44:13] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 08:45:17,498 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 08:45:18,820 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:45:27,712 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:45:30,263 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:45:30,266 - __main__ - INFO - Starting application...
2024-12-26 08:45:30,266 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:45:30,267 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:45:30,267 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:45:30,295 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:45:30,301 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:45:30,333 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:45:30] "GET / HTTP/1.1" 200 -
2024-12-26 08:45:30,515 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:45:30] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 08:45:30,523 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:45:30] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 08:45:36,743 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:45:37,837 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:46:45,624 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:46:48,021 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:46:48,025 - __main__ - INFO - Starting application...
2024-12-26 08:46:48,026 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:46:48,026 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:46:48,027 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:46:48,059 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 08:46:48,059 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 08:46:48,061 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:46:55,775 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:46:56,415 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:46:56,929 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:46:58,580 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:46:58,584 - __main__ - INFO - Starting application...
2024-12-26 08:46:58,584 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:46:58,584 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:46:58,584 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:46:58,593 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:46:58,597 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:46:58,623 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:46:58] "GET / HTTP/1.1" 200 -
2024-12-26 08:46:58,772 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:46:58] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 08:46:58,774 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:46:58] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 08:47:05,344 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:47:07,265 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:48:39,423 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:48:39] "GET / HTTP/1.1" 200 -
2024-12-26 08:48:39,502 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:48:39] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 08:48:39,502 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:48:39] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 08:49:16,860 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:49:16] "GET / HTTP/1.1" 200 -
2024-12-26 08:49:16,946 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:49:16] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 08:49:16,946 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:49:16] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 08:49:21,124 - __main__ - INFO - File saved successfully: 5f4d0aa7b18d886fcbc9fd2053d2126fe1d74b2d57f45af48ac866dde75373c4
2024-12-26 08:49:21,160 - __main__ - INFO - Temporary file removed: 5f4d0aa7b18d886fcbc9fd2053d2126fe1d74b2d57f45af48ac866dde75373c4
2024-12-26 08:49:21,160 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:49:21] "POST /upload HTTP/1.1" 200 -
2024-12-26 08:49:35,394 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:49:35] "POST /summarize HTTP/1.1" 200 -
2024-12-26 08:56:03,787 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:56:09,668 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:56:09,670 - __main__ - INFO - Starting application...
2024-12-26 08:56:09,670 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:56:09,671 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:56:09,671 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:56:09,685 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 08:56:09,685 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 08:56:09,687 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:56:15,857 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:56:16,892 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:56:17,788 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:56:20,631 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:56:20,634 - __main__ - INFO - Starting application...
2024-12-26 08:56:20,634 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:56:20,634 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:56:20,634 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:56:20,645 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:56:20,648 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:56:24,930 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:24] "GET / HTTP/1.1" 200 -
2024-12-26 08:56:25,074 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:25] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 08:56:25,076 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:25] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 08:56:27,412 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:56:28,919 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:56:29,106 - __main__ - INFO - File saved successfully: a373e6f424e5c076205c793a928fa2a0e97644cde7680df86cd09b872bf7e9c9
2024-12-26 08:56:29,142 - __main__ - INFO - Temporary file removed: a373e6f424e5c076205c793a928fa2a0e97644cde7680df86cd09b872bf7e9c9
2024-12-26 08:56:29,144 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:29] "POST /upload HTTP/1.1" 200 -
2024-12-26 08:56:33,519 - __main__ - WARNING - Malicious input detected in question: select * from users
2024-12-26 08:56:33,519 - __main__ - WARNING - Malicious input detected in question: select * from users
2024-12-26 08:56:33,519 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:33] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 08:56:33,520 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:33] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 08:56:46,494 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:46] "POST /summarize HTTP/1.1" 200 -
2024-12-26 08:56:47,773 - __main__ - WARNING - Malicious input detected in question: ? . ,
2024-12-26 08:56:47,774 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:47] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 08:56:48,873 - __main__ - WARNING - Malicious input detected in question: ? . ,
2024-12-26 08:56:48,874 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:48] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 08:56:50,345 - __main__ - WARNING - Malicious input detected in question: ? . ,
2024-12-26 08:56:50,345 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:50] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 08:56:52,704 - __main__ - WARNING - Malicious input detected in question: ? . ,
2024-12-26 08:56:52,704 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:52] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 08:56:56,892 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:56:56,893 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:56:56] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:57:22,169 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 08:57:23,223 - werkzeug - INFO -  * Restarting with stat
2024-12-26 08:57:32,451 - __main__ - INFO - Successfully loaded QA model
2024-12-26 08:57:34,991 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 08:57:34,994 - __main__ - INFO - Starting application...
2024-12-26 08:57:34,994 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 08:57:34,995 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 08:57:34,995 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 08:57:35,005 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 08:57:35,009 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 08:57:35,030 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:35,030 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:35,033 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:35] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:35,035 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:35] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:42,464 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 08:57:42,612 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:42,613 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:42] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:42,614 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:42,615 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:42] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:43,235 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:43,236 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:43] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:43,237 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:43,237 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:43] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:43,506 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:43,506 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:43] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:43,507 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:43,508 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:43] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:43,690 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:43,690 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:43] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:43,692 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:43,692 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:43] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:44,033 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 08:57:47,047 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:47,047 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:47,047 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:47] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:47,050 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:47] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:53,414 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:53,414 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:53] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:53,415 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:53,416 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:53] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:54,217 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:54,218 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:54] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:54,219 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:54,220 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:54] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:55,737 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:55,738 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:55] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:56,180 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:56,181 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:56] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:56,361 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:56,362 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:56] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:56,528 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 08:57:56,529 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:56] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 08:57:58,460 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:58] "GET / HTTP/1.1" 200 -
2024-12-26 08:57:58,642 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:58] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 08:57:58,648 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:57:58] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 08:58:02,450 - __main__ - INFO - File saved successfully: 6e3a7674b75293d4b7885021d3f7cd96ebcf1b06352f4c535da86bce046a1013
2024-12-26 08:58:02,495 - __main__ - INFO - Temporary file removed: 6e3a7674b75293d4b7885021d3f7cd96ebcf1b06352f4c535da86bce046a1013
2024-12-26 08:58:02,495 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:02] "POST /upload HTTP/1.1" 200 -
2024-12-26 08:58:06,552 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:06] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:06,553 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:06] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:10,346 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:58:10,346 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:10,361 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:58:10,363 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:12,626 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:58:12,627 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:12] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:12,636 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:58:12,637 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:12] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:14,568 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:58:14,569 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:14] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:14,569 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:58:14,570 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:14] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:20,193 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:58:20,195 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:20] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:20,200 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 08:58:20,201 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:20] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:31,971 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:31] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:31,991 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:31] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:45,554 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:45] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:58:45,558 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:58:45] "POST /ask HTTP/1.1" 200 -
2024-12-26 08:59:01,808 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 08:59:01] "POST /summarize HTTP/1.1" 200 -
2024-12-26 08:59:16,924 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 08:59:18,480 - werkzeug - INFO -  * Restarting with stat
2024-12-26 09:00:24,282 - __main__ - INFO - Successfully loaded QA model
2024-12-26 09:00:26,488 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 09:00:26,491 - __main__ - INFO - Starting application...
2024-12-26 09:00:26,492 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 09:00:26,492 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 09:00:26,492 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 09:00:26,507 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 09:00:26,508 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 09:00:26,511 - werkzeug - INFO -  * Restarting with stat
2024-12-26 09:00:34,496 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 09:00:35,600 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 09:00:36,032 - __main__ - INFO - Successfully loaded QA model
2024-12-26 09:00:40,092 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 09:00:40,094 - __main__ - INFO - Starting application...
2024-12-26 09:00:40,094 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 09:00:40,095 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 09:00:40,095 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 09:00:40,104 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 09:00:40,109 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 09:00:45,797 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 09:00:46,890 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 09:00:52,095 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:00:52] "GET / HTTP/1.1" 200 -
2024-12-26 09:00:52,266 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:00:52] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 09:00:52,280 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:00:52] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 09:01:00,828 - __main__ - INFO - File saved successfully: 5875d2f688878b535fd63f7342e73fa2067e678f3b7eb8b2b8f5e5d9a8f3bbff
2024-12-26 09:01:00,869 - __main__ - INFO - Temporary file removed: 5875d2f688878b535fd63f7342e73fa2067e678f3b7eb8b2b8f5e5d9a8f3bbff
2024-12-26 09:01:00,870 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:00] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:01:03,182 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:01:03,183 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:01:03,184 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:03] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:01:03,184 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:03] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:01:17,523 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 09:01:18,629 - werkzeug - INFO -  * Restarting with stat
2024-12-26 09:01:27,243 - __main__ - INFO - Successfully loaded QA model
2024-12-26 09:01:29,079 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 09:01:29,083 - __main__ - INFO - Starting application...
2024-12-26 09:01:29,083 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 09:01:29,083 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 09:01:29,084 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 09:01:29,093 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 09:01:29,101 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 09:01:30,780 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 09:01:30,781 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 09:01:30,783 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:30] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 09:01:30,784 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:30] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 09:01:33,625 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 09:01:33,625 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:33] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 09:01:33,626 - chatbot - WARNING - Forbidden (CSRF token missing or incorrect.): /ask
2024-12-26 09:01:33,627 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:33] "[31m[1mPOST /ask HTTP/1.1[0m" 403 -
2024-12-26 09:01:36,124 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 09:01:36,216 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:36] "GET / HTTP/1.1" 200 -
2024-12-26 09:01:36,339 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:36] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 09:01:36,347 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:36] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 09:01:38,011 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 09:01:39,635 - __main__ - INFO - File saved successfully: 0c7314e16992a6c8fc937532a449e2939f5186d0e0846faa3d6e9cfc2115a970
2024-12-26 09:01:39,680 - __main__ - INFO - Temporary file removed: 0c7314e16992a6c8fc937532a449e2939f5186d0e0846faa3d6e9cfc2115a970
2024-12-26 09:01:39,680 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:39] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:01:44,511 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:01:44,512 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:44] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:01:44,528 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:01:44,529 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:44] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:01:47,643 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:01:47,644 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:47] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:01:47,648 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:01:47,649 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:01:47] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:01:56,422 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 09:01:57,551 - werkzeug - INFO -  * Restarting with stat
2024-12-26 09:05:07,612 - __main__ - INFO - Successfully loaded QA model
2024-12-26 09:05:10,297 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 09:05:10,300 - __main__ - INFO - Starting application...
2024-12-26 09:05:10,303 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 09:05:10,304 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 09:05:10,307 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 09:05:10,325 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 09:05:10,325 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 09:05:10,329 - werkzeug - INFO -  * Restarting with stat
2024-12-26 09:05:17,849 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 09:05:18,840 - __main__ - INFO - Successfully loaded QA model
2024-12-26 09:05:19,318 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 09:05:21,164 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 09:05:21,167 - __main__ - INFO - Starting application...
2024-12-26 09:05:21,167 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 09:05:21,167 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 09:05:21,167 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 09:05:21,176 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 09:05:21,178 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 09:05:26,132 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:05:26] "GET / HTTP/1.1" 200 -
2024-12-26 09:05:26,261 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:05:26] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 09:05:26,277 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:05:26] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 09:05:28,414 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 09:05:29,894 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 09:05:30,680 - __main__ - INFO - File saved successfully: 88cd039633472c12b3f328e77eb6d9ad68eb4c744283ea808a28098e5a5fc76d
2024-12-26 09:05:30,731 - __main__ - INFO - Temporary file removed: 88cd039633472c12b3f328e77eb6d9ad68eb4c744283ea808a28098e5a5fc76d
2024-12-26 09:05:30,732 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:05:30] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:05:35,822 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:05:35] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:05:35,822 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:05:35] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:05:38,116 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:05:38,116 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:05:38] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:05:38,116 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:05:38,116 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:05:38] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:06:06,545 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 09:06:07,610 - werkzeug - INFO -  * Restarting with stat
2024-12-26 09:06:16,649 - __main__ - INFO - Successfully loaded QA model
2024-12-26 09:06:21,049 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 09:06:21,049 - __main__ - INFO - Starting application...
2024-12-26 09:06:21,053 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 09:06:21,053 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 09:06:21,054 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 09:06:21,061 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 09:06:21,066 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 09:06:27,653 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 09:06:27,924 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:06:27] "GET / HTTP/1.1" 200 -
2024-12-26 09:06:28,084 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:06:28] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 09:06:28,107 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:06:28] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 09:06:29,538 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 09:06:32,127 - __main__ - INFO - File saved successfully: f89b5751f7e2f8d4ccb5186fd5de505802d8436b5c5425ed178a620f477fc841
2024-12-26 09:06:32,168 - __main__ - INFO - Temporary file removed: f89b5751f7e2f8d4ccb5186fd5de505802d8436b5c5425ed178a620f477fc841
2024-12-26 09:06:32,168 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:06:32] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:06:37,511 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:06:37] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:06:37,518 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:06:37] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:06:39,329 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:06:39,329 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:06:39] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:06:39,333 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 09:06:39,334 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:06:39] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:07:32,724 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 09:07:33,905 - werkzeug - INFO -  * Restarting with stat
2024-12-26 09:07:42,320 - __main__ - INFO - Successfully loaded QA model
2024-12-26 09:07:45,618 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 09:07:45,619 - __main__ - INFO - Starting application...
2024-12-26 09:07:45,619 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 09:07:45,619 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 09:07:45,619 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 09:07:45,633 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 09:07:45,636 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 09:07:52,454 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 09:07:54,348 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 09:07:55,722 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 09:08:05,528 - werkzeug - INFO -  * Restarting with stat
2024-12-26 09:08:13,217 - __main__ - INFO - Successfully loaded QA model
2024-12-26 09:08:15,720 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 09:08:15,723 - __main__ - INFO - Starting application...
2024-12-26 09:08:15,724 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 09:08:15,724 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 09:08:15,724 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 09:08:15,730 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 09:08:15,734 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 09:08:18,651 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:08:18] "GET / HTTP/1.1" 200 -
2024-12-26 09:08:18,823 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:08:18] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 09:08:18,824 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:08:18] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 09:08:22,152 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 09:08:22,685 - __main__ - INFO - File saved successfully: 4a3a87d09ee3cdb8dab77aaf43c7cf428674580aae48b0da99917e5c137bb911
2024-12-26 09:08:22,716 - __main__ - INFO - Temporary file removed: 4a3a87d09ee3cdb8dab77aaf43c7cf428674580aae48b0da99917e5c137bb911
2024-12-26 09:08:22,717 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:08:22] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:08:23,212 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 09:08:28,175 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:08:28] "POST /ask HTTP/1.1" 200 -
2024-12-26 09:33:44,106 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:33:44] "GET / HTTP/1.1" 200 -
2024-12-26 09:33:44,164 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:33:44] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 09:33:44,165 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:33:44] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 09:33:44,184 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:33:44] "[33mGET /daytime-background.jpg HTTP/1.1[0m" 404 -
2024-12-26 09:34:09,267 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:34:09] "GET / HTTP/1.1" 200 -
2024-12-26 09:34:09,316 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:34:09] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 09:34:09,317 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:34:09] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 09:34:09,324 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:34:09] "[33mGET /daytime-background.jpg HTTP/1.1[0m" 404 -
2024-12-26 09:37:44,862 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:37:44] "GET / HTTP/1.1" 200 -
2024-12-26 09:37:44,919 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:37:44] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 09:37:44,919 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:37:44] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 09:37:49,453 - __main__ - INFO - File saved successfully: 15b93b62dbd22571f2a801dbab7da58f5a88410b1eb40fd178aa2167ec6fc0ad
2024-12-26 09:37:49,477 - __main__ - INFO - Temporary file removed: 15b93b62dbd22571f2a801dbab7da58f5a88410b1eb40fd178aa2167ec6fc0ad
2024-12-26 09:37:49,478 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:37:49] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:38:15,313 - __main__ - INFO - File saved successfully: a2ef2b45536b3c093e5ff86ea838da9d97b5332919396f86800c4443d799a77b
2024-12-26 09:38:15,452 - __main__ - INFO - Temporary file removed: a2ef2b45536b3c093e5ff86ea838da9d97b5332919396f86800c4443d799a77b
2024-12-26 09:38:15,453 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:38:15] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:38:23,794 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:38:23] "POST /summarize HTTP/1.1" 200 -
2024-12-26 09:38:40,153 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:38:40] "GET / HTTP/1.1" 200 -
2024-12-26 09:38:40,200 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:38:40] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 09:38:40,201 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:38:40] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 09:38:45,576 - __main__ - INFO - File saved successfully: 07eb6d1d2a72b904923a1c308998d6d520c2a500f93baf206cb1ce2277048aa8
2024-12-26 09:38:45,594 - __main__ - INFO - Temporary file removed: 07eb6d1d2a72b904923a1c308998d6d520c2a500f93baf206cb1ce2277048aa8
2024-12-26 09:38:45,595 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:38:45] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:38:51,937 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:38:51] "POST /summarize HTTP/1.1" 200 -
2024-12-26 09:50:41,500 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:50:41] "GET / HTTP/1.1" 200 -
2024-12-26 09:50:41,546 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:50:41] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 09:50:41,547 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:50:41] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 09:50:45,248 - __main__ - INFO - File saved successfully: a3b1c14e488b59d0cdd16288a999de92c9fadf943590f4ee85441966aaee9614
2024-12-26 09:50:45,252 - __main__ - INFO - Temporary file removed: a3b1c14e488b59d0cdd16288a999de92c9fadf943590f4ee85441966aaee9614
2024-12-26 09:50:45,252 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:50:45] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:52:34,590 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:52:34] "GET / HTTP/1.1" 200 -
2024-12-26 09:52:34,641 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:52:34] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 09:52:34,642 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:52:34] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 09:52:38,305 - __main__ - INFO - File saved successfully: 9cee4a636089b9e990a62824aeebb3fa6145bf153f172803307cefb5a7e467a3
2024-12-26 09:52:38,331 - __main__ - INFO - Temporary file removed: 9cee4a636089b9e990a62824aeebb3fa6145bf153f172803307cefb5a7e467a3
2024-12-26 09:52:38,331 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:52:38] "POST /upload HTTP/1.1" 200 -
2024-12-26 09:52:50,584 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:52:50] "GET / HTTP/1.1" 200 -
2024-12-26 09:52:50,641 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:52:50] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 09:52:50,641 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:52:50] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 09:54:22,070 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:54:22] "GET / HTTP/1.1" 200 -
2024-12-26 09:54:22,126 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:54:22] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 09:54:22,127 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 09:54:22] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 10:14:04,921 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:14:04] "GET / HTTP/1.1" 200 -
2024-12-26 10:14:04,975 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:14:04] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:14:04,976 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:14:04] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:14:04,977 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:14:04] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:14:04,980 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:14:04] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:14:04,983 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:14:04] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:15:50,689 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:50] "GET / HTTP/1.1" 200 -
2024-12-26 10:15:50,744 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:50] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:15:50,745 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:50] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:50,746 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:50] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:50,749 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:50] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:50,755 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:50] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,419 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "GET / HTTP/1.1" 200 -
2024-12-26 10:15:51,467 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,468 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,470 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,471 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,473 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,668 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "GET / HTTP/1.1" 200 -
2024-12-26 10:15:51,727 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,729 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,730 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,733 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,737 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,869 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "GET / HTTP/1.1" 200 -
2024-12-26 10:15:51,922 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,923 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,925 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,928 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:51,929 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:51] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,055 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "GET / HTTP/1.1" 200 -
2024-12-26 10:15:52,104 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,106 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,108 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,110 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,113 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,220 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "GET / HTTP/1.1" 200 -
2024-12-26 10:15:52,268 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,269 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,271 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,275 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,278 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,824 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "GET / HTTP/1.1" 200 -
2024-12-26 10:15:52,870 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,873 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,875 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,877 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:15:52,880 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:15:52] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:16:10,928 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:16:13,361 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:16:13,364 - __main__ - INFO - Starting application...
2024-12-26 10:16:13,364 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:16:13,364 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:16:13,364 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:16:13,393 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 10:16:13,394 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 10:16:13,396 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:16:17,931 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:16:18,891 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:16:19,683 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:16:20,434 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:16:20,435 - __main__ - INFO - Starting application...
2024-12-26 10:16:20,435 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:16:20,436 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:16:20,436 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:16:20,441 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:16:20,458 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:16:20,473 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:16:20] "GET / HTTP/1.1" 200 -
2024-12-26 10:16:20,527 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:16:20] "[33mGET /style.css HTTP/1.1[0m" 404 -
2024-12-26 10:16:20,529 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:16:20] "[33mGET /photo1_url HTTP/1.1[0m" 404 -
2024-12-26 10:16:20,530 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:16:20] "[33mGET /photo2_url HTTP/1.1[0m" 404 -
2024-12-26 10:16:20,534 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:16:20] "[33mGET /photo3_url HTTP/1.1[0m" 404 -
2024-12-26 10:16:20,535 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:16:20] "[33mGET /main.js HTTP/1.1[0m" 404 -
2024-12-26 10:16:24,828 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:16:25,788 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:18:00,792 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:18:00] "GET / HTTP/1.1" 200 -
2024-12-26 10:18:00,897 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:18:00] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 10:18:00,898 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:18:00] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 10:18:04,724 - __main__ - INFO - File saved successfully: 883914da4067ee93327b9d1c7382a19fc8224d916d9d1cab551161b0185f13f1
2024-12-26 10:18:04,745 - __main__ - INFO - Temporary file removed: 883914da4067ee93327b9d1c7382a19fc8224d916d9d1cab551161b0185f13f1
2024-12-26 10:18:04,746 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:18:04] "POST /upload HTTP/1.1" 200 -
2024-12-26 10:18:13,399 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:18:13] "POST /summarize HTTP/1.1" 200 -
2024-12-26 10:34:24,590 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:34:27,862 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:34:27,864 - __main__ - INFO - Starting application...
2024-12-26 10:34:27,864 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:34:27,864 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:34:27,865 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:34:27,874 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 10:34:27,876 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 10:34:27,877 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:34:32,075 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:34:33,186 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:34:34,111 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:34:36,691 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:34:36,693 - __main__ - INFO - Starting application...
2024-12-26 10:34:36,693 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:34:36,694 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:34:36,694 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:34:36,698 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:34:36,702 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:34:36,730 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:36] "GET / HTTP/1.1" 200 -
2024-12-26 10:34:36,789 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:36] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 10:34:36,796 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:36] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 10:34:36,823 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:36] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2024-12-26 10:34:39,086 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:39] "GET /contributors HTTP/1.1" 200 -
2024-12-26 10:34:39,108 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:39] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:34:39,109 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:39] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:34:40,749 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:34:41,888 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:34:43,593 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:43] "GET / HTTP/1.1" 200 -
2024-12-26 10:34:43,610 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:43] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:34:43,612 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:34:43] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:40:04,301 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:40:05,032 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:40:10,655 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:40:12,940 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:40:17,902 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:40:19,378 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:40:53,103 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:40:54,588 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:41:00,051 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:41:01,048 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:41:19,363 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:41:20,787 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:41:20,789 - __main__ - INFO - Starting application...
2024-12-26 10:41:20,789 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:41:20,790 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:41:20,790 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:41:20,799 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 10:41:20,799 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 10:41:20,800 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:41:25,507 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:41:26,430 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:41:26,711 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:41:27,920 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:41:27,923 - __main__ - INFO - Starting application...
2024-12-26 10:41:27,923 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:41:27,923 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:41:27,924 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:41:27,929 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:41:27,933 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:41:27,953 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:27] "GET / HTTP/1.1" 200 -
2024-12-26 10:41:28,011 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:28] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:41:28,021 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:28] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:41:30,421 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:30] "GET /contributors HTTP/1.1" 200 -
2024-12-26 10:41:30,439 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:30] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:41:30,446 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:30] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:41:32,741 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:41:33,778 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:41:33,805 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:33] "GET / HTTP/1.1" 200 -
2024-12-26 10:41:33,828 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:33] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:41:33,829 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:33] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:41:36,011 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:36] "GET / HTTP/1.1" 200 -
2024-12-26 10:41:36,024 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:36] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:41:36,027 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:41:36] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:42:09,164 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:42:09,890 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:42:16,213 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:42:18,651 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:42:18,654 - __main__ - INFO - Starting application...
2024-12-26 10:42:18,654 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:42:18,654 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:42:18,654 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:42:18,658 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:42:18,661 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:42:20,375 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:42:20] "GET / HTTP/1.1" 200 -
2024-12-26 10:42:20,538 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:42:20] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 10:42:20,539 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:42:20] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 10:42:23,654 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:42:24,669 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:42:34,606 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:42:34] "[31m[1mPOST /ask HTTP/1.1[0m" 400 -
2024-12-26 10:43:15,957 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:43:16,661 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:43:22,439 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:43:25,454 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:43:25,456 - __main__ - INFO - Starting application...
2024-12-26 10:43:25,457 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:43:25,457 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:43:25,457 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:43:25,462 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:43:25,466 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:43:25,486 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:43:25] "GET / HTTP/1.1" 200 -
2024-12-26 10:43:25,621 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:43:25] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:43:25,625 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:43:25] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:43:30,810 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:43:33,098 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:49:55,013 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:55] "GET / HTTP/1.1" 200 -
2024-12-26 10:49:55,070 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:55] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 10:49:55,071 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:55] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 10:49:57,228 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:57] "GET /contributors HTTP/1.1" 200 -
2024-12-26 10:49:57,284 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:57] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:49:57,284 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:57] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:49:59,101 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:59] "GET / HTTP/1.1" 200 -
2024-12-26 10:49:59,159 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:59] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:49:59,161 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:49:59] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:52:00,003 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:52:00,730 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:52:06,669 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:52:08,836 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:52:08,838 - __main__ - INFO - Starting application...
2024-12-26 10:52:08,838 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:52:08,838 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:52:08,838 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:52:08,843 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:52:08,846 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:52:13,412 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:52:14,496 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:52:51,069 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:52:51,856 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:52:57,334 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:52:59,191 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:52:59,194 - __main__ - INFO - Starting application...
2024-12-26 10:52:59,195 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:52:59,195 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:52:59,195 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:52:59,199 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:52:59,202 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:53:03,852 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:53:04,951 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:53:06,538 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:53:06] "GET / HTTP/1.1" 200 -
2024-12-26 10:53:06,594 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:53:06] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 10:53:06,595 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:53:06] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 10:53:18,308 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:53:19,025 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:53:24,555 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:53:26,212 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:53:26,213 - __main__ - INFO - Starting application...
2024-12-26 10:53:26,213 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:53:26,214 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:53:26,214 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:53:26,218 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:53:26,222 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:53:26,239 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:53:26] "GET / HTTP/1.1" 200 -
2024-12-26 10:53:26,316 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:53:26] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:53:26,317 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:53:26] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:53:31,360 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:53:32,804 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:54:28,548 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:54:29,305 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:54:34,946 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:54:36,378 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:54:36,380 - __main__ - INFO - Starting application...
2024-12-26 10:54:36,380 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:54:36,381 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:54:36,381 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:54:36,386 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:54:36,388 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:54:36,403 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:54:36] "GET / HTTP/1.1" 200 -
2024-12-26 10:54:36,450 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:54:36] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:54:36,459 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:54:36] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:54:41,299 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:54:42,312 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:54:46,451 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:54:53,628 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:54:58,965 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:55:00,416 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:55:00,418 - __main__ - INFO - Starting application...
2024-12-26 10:55:00,418 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:55:00,418 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:55:00,418 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:55:00,424 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:55:00,428 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:55:05,487 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:55:06,956 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 10:57:50,492 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:57:50] "GET / HTTP/1.1" 200 -
2024-12-26 10:57:50,541 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:57:50] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:57:50,545 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:57:50] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 10:57:51,269 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:57:51] "GET / HTTP/1.1" 200 -
2024-12-26 10:57:51,280 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:57:51] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 10:57:51,284 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:57:51] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 10:58:06,867 - __main__ - INFO - File saved successfully: 092c79c34710a298f8d29c18ec194ad01e164c985ed3ccb0d2e553714fe86bfe
2024-12-26 10:58:06,890 - __main__ - INFO - Temporary file removed: 092c79c34710a298f8d29c18ec194ad01e164c985ed3ccb0d2e553714fe86bfe
2024-12-26 10:58:06,891 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:58:06] "POST /upload HTTP/1.1" 200 -
2024-12-26 10:58:10,537 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 10:58:10,537 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 10:58:10,537 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:58:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 10:58:10,538 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:58:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 10:58:19,794 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:58:19] "POST /ask HTTP/1.1" 200 -
2024-12-26 10:58:19,814 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:58:19] "POST /ask HTTP/1.1" 200 -
2024-12-26 10:58:31,935 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:58:31] "POST /summarize HTTP/1.1" 200 -
2024-12-26 10:58:48,823 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 10:58:48,824 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:58:48] "POST /ask HTTP/1.1" 200 -
2024-12-26 10:58:48,826 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 10:58:48,826 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:58:48] "POST /ask HTTP/1.1" 200 -
2024-12-26 10:59:00,025 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 10:59:00,026 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:59:00] "POST /ask HTTP/1.1" 200 -
2024-12-26 10:59:00,034 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 10:59:00,035 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:59:00] "POST /ask HTTP/1.1" 200 -
2024-12-26 10:59:21,747 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 10:59:22,538 - werkzeug - INFO -  * Restarting with stat
2024-12-26 10:59:28,644 - __main__ - INFO - Successfully loaded QA model
2024-12-26 10:59:30,234 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 10:59:30,236 - __main__ - INFO - Starting application...
2024-12-26 10:59:30,236 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 10:59:30,236 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 10:59:30,236 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 10:59:30,243 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 10:59:30,248 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 10:59:30,266 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:59:30] "GET / HTTP/1.1" 200 -
2024-12-26 10:59:30,378 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:59:30] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-26 10:59:30,380 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 10:59:30] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 10:59:34,869 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 10:59:35,878 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:00:03,439 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:00:04,193 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:00:10,230 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:00:13,226 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:00:13,227 - __main__ - INFO - Starting application...
2024-12-26 11:00:13,228 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:00:13,228 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:00:13,228 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:00:13,233 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:00:13,236 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:00:13,257 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:00:13] "GET / HTTP/1.1" 200 -
2024-12-26 11:00:13,366 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:00:13] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:00:13,374 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:00:13] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 11:00:16,759 - __main__ - INFO - File saved successfully: a5d3a3419af5ed91bb12d0680727e686f8521054e2cdd521014fb669a3669236
2024-12-26 11:00:16,785 - __main__ - INFO - Temporary file removed: a5d3a3419af5ed91bb12d0680727e686f8521054e2cdd521014fb669a3669236
2024-12-26 11:00:16,785 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:00:16] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:00:17,909 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:00:19,401 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:00:27,752 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:00:27] "POST /summarize HTTP/1.1" 200 -
2024-12-26 11:01:34,683 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:01:35,491 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:01:41,448 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:01:43,366 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:01:43,368 - __main__ - INFO - Starting application...
2024-12-26 11:01:43,368 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:01:43,369 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:01:43,369 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:01:43,374 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:01:43,377 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:01:47,763 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:01:47] "GET / HTTP/1.1" 200 -
2024-12-26 11:01:47,826 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:01:47] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:01:47,829 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:01:47] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 11:01:48,725 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:01:50,703 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:01:51,566 - __main__ - INFO - File saved successfully: e93b08e7c2f87449e6f22edfb6d5ea81231027a1b131134268e45fbb194156e4
2024-12-26 11:01:51,591 - __main__ - INFO - Temporary file removed: e93b08e7c2f87449e6f22edfb6d5ea81231027a1b131134268e45fbb194156e4
2024-12-26 11:01:51,592 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:01:51] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:01:57,774 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:01:57] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:02:05,814 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:02:05] "POST /summarize HTTP/1.1" 200 -
2024-12-26 11:03:13,855 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:03:14,625 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:06:28,533 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:06:30,002 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:06:30,004 - __main__ - INFO - Starting application...
2024-12-26 11:06:30,004 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:06:30,004 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:06:30,005 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:06:30,015 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 11:06:30,016 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 11:06:30,017 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:06:35,159 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:06:35,684 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:06:37,008 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:06:38,439 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:06:38,444 - __main__ - INFO - Starting application...
2024-12-26 11:06:38,444 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:06:38,445 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:06:38,445 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:06:38,454 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:06:38,458 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:06:38,477 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:06:38] "GET / HTTP/1.1" 200 -
2024-12-26 11:06:38,556 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:06:38] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:06:38,558 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:06:38] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 11:06:42,870 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:06:43,933 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:06:44,743 - __main__ - INFO - File saved successfully: 1a158e2d9c7d2ffb28446e08f58fdb2277233e0f76250d2d6c6bd53c2033f40a
2024-12-26 11:06:44,770 - __main__ - INFO - Temporary file removed: 1a158e2d9c7d2ffb28446e08f58fdb2277233e0f76250d2d6c6bd53c2033f40a
2024-12-26 11:06:44,771 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:06:44] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:06:48,320 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:06:48,321 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:06:48] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:06:54,911 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:06:54,911 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:06:54] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:06:54,915 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:06:54,915 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:06:54] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:07:44,798 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:07:45,555 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:07:54,456 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:07:55,930 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:07:55,933 - __main__ - INFO - Starting application...
2024-12-26 11:07:55,933 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:07:55,933 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:07:55,933 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:07:55,943 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 11:07:55,943 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 11:07:55,945 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:08:01,072 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:08:01,726 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:08:02,933 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:08:03,527 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:08:03,529 - __main__ - INFO - Starting application...
2024-12-26 11:08:03,529 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:08:03,529 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:08:03,530 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:08:03,535 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:08:03,539 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:08:03,558 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:03] "GET / HTTP/1.1" 200 -
2024-12-26 11:08:03,622 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:03] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:08:03,634 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:03] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 11:08:08,342 - __main__ - INFO - File saved successfully: 6e3d054e66436c2e030fda103ed809873d49cf7e71b37e81c5ea077957e58993
2024-12-26 11:08:08,366 - __main__ - INFO - Temporary file removed: 6e3d054e66436c2e030fda103ed809873d49cf7e71b37e81c5ea077957e58993
2024-12-26 11:08:08,366 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:08] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:08:08,780 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:08:10,117 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:08:10,117 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:08:10,118 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:08:10,118 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:08:10,671 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:08:17,636 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:08:18,403 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:08:27,523 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:08:28,979 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:08:28,980 - __main__ - INFO - Starting application...
2024-12-26 11:08:28,980 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:08:28,981 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:08:28,981 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:08:28,990 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 11:08:28,990 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 11:08:28,991 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:08:33,837 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:08:34,741 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:08:34,837 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:08:36,541 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:08:36,543 - __main__ - INFO - Starting application...
2024-12-26 11:08:36,543 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:08:36,544 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:08:36,544 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:08:36,549 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:08:36,553 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:08:36,570 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:36] "GET / HTTP/1.1" 200 -
2024-12-26 11:08:36,707 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:36] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 11:08:36,708 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:36] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:08:39,862 - __main__ - INFO - File saved successfully: 98df5352a3663871e6011c8a991af6c410135405114e262278c358fa59b1f85d
2024-12-26 11:08:39,890 - __main__ - INFO - Temporary file removed: 98df5352a3663871e6011c8a991af6c410135405114e262278c358fa59b1f85d
2024-12-26 11:08:39,890 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:39] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:08:41,664 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:08:41,664 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:41] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:08:41,667 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:08:41,668 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:08:41] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:08:41,793 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:08:43,233 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:08:56,678 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:08:58,686 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:09:04,388 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:09:05,891 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:09:05,893 - __main__ - INFO - Starting application...
2024-12-26 11:09:05,893 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:09:05,894 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:09:05,894 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:09:05,899 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:09:05,902 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:09:05,918 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:09:05] "GET / HTTP/1.1" 200 -
2024-12-26 11:09:05,980 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:09:05] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:09:05,980 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:09:05] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 11:09:08,795 - __main__ - INFO - File saved successfully: 987d8719e3169e424ab91cb723ce63bd388652a107a51fe954b864e5993e3e55
2024-12-26 11:09:08,817 - __main__ - INFO - Temporary file removed: 987d8719e3169e424ab91cb723ce63bd388652a107a51fe954b864e5993e3e55
2024-12-26 11:09:08,818 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:09:08] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:09:10,770 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:09:10,770 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:09:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:09:10,771 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:09:10,771 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:09:10] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:09:12,228 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:09:13,289 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:09:26,420 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:09:26] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:09:33,947 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:09:33,948 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:09:33] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:10:04,216 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:10:04,982 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:10:10,634 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:10:12,052 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:10:12,053 - __main__ - INFO - Starting application...
2024-12-26 11:10:12,054 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:10:12,054 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:10:12,054 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:10:12,059 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:10:12,062 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:10:17,193 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:10:18,626 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:11:44,511 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:11:45,256 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:12:07,721 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:12:09,139 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:12:09,140 - __main__ - INFO - Starting application...
2024-12-26 11:12:09,140 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:12:09,141 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:12:09,141 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:12:09,150 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-26 11:12:09,150 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-26 11:12:09,152 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:12:13,612 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:12:14,466 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:12:14,623 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:12:15,879 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:12:15,881 - __main__ - INFO - Starting application...
2024-12-26 11:12:15,881 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:12:15,881 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:12:15,881 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:12:15,887 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:12:15,890 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:12:20,505 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:12:21,931 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:12:22,473 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:12:22] "GET / HTTP/1.1" 200 -
2024-12-26 11:12:22,547 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:12:22] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 11:12:22,548 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:12:22] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:12:25,600 - __main__ - INFO - File saved successfully: 20a87ccb690df0e3ccf1b0cc66e9cada2ffb91a93ebba452cfeff623646a3797
2024-12-26 11:12:25,625 - __main__ - INFO - Temporary file removed: 20a87ccb690df0e3ccf1b0cc66e9cada2ffb91a93ebba452cfeff623646a3797
2024-12-26 11:12:25,626 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:12:25] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:12:27,713 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:12:27,714 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:12:27,714 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:12:27] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:12:27,715 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:12:27] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:12:43,033 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:12:43,780 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:12:49,218 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:12:50,704 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:12:50,706 - __main__ - INFO - Starting application...
2024-12-26 11:12:50,707 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:12:50,707 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:12:50,707 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:12:50,713 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:12:50,717 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:12:55,414 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:12:56,445 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:13:04,442 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:13:04] "GET / HTTP/1.1" 200 -
2024-12-26 11:13:04,556 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:13:04] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 11:13:04,558 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:13:04] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:13:07,604 - __main__ - INFO - File saved successfully: 87c3c542225e10387c3c0e9aa1b17263320ee6715c03d4f016d624591e61697c
2024-12-26 11:13:07,626 - __main__ - INFO - Temporary file removed: 87c3c542225e10387c3c0e9aa1b17263320ee6715c03d4f016d624591e61697c
2024-12-26 11:13:07,627 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:13:07] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:13:09,453 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:13:09,454 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:13:09,454 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:13:09] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:13:09,454 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:13:09] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:14:09,808 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:09] "GET / HTTP/1.1" 200 -
2024-12-26 11:14:09,825 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:09] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:14:09,825 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:09] "GET /static/js/main.js HTTP/1.1" 200 -
2024-12-26 11:14:13,236 - __main__ - INFO - File saved successfully: 01a6139b60ef71cc828f3aa771474f9529c318a6bd3d49cba2a37f146d184de7
2024-12-26 11:14:13,263 - __main__ - INFO - Temporary file removed: 01a6139b60ef71cc828f3aa771474f9529c318a6bd3d49cba2a37f146d184de7
2024-12-26 11:14:13,264 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:13] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:14:15,463 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:14:15,464 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:15] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:14:15,470 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:14:15,471 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:15] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:14:20,936 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:20] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:14:20,938 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:20] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:14:31,221 - werkzeug - INFO -  * Detected change in 'D:\\Chatbot\\Chatbot\\chatbot_system\\backend\\chatbot.py', reloading
2024-12-26 11:14:31,983 - werkzeug - INFO -  * Restarting with stat
2024-12-26 11:14:37,412 - __main__ - INFO - Successfully loaded QA model
2024-12-26 11:14:39,309 - __main__ - INFO - Successfully loaded summarization model
2024-12-26 11:14:39,310 - __main__ - INFO - Starting application...
2024-12-26 11:14:39,310 - __main__ - INFO - Template folder: D:\Chatbot\Chatbot\chatbot_system\frontend\templates
2024-12-26 11:14:39,312 - __main__ - INFO - Static folder: D:\Chatbot\Chatbot\chatbot_system\frontend\static
2024-12-26 11:14:39,312 - __main__ - INFO - Upload folder: D:\Chatbot\Chatbot\chatbot_system\temp
2024-12-26 11:14:39,316 - werkzeug - WARNING -  * Debugger is active!
2024-12-26 11:14:39,319 - werkzeug - INFO -  * Debugger PIN: 921-300-071
2024-12-26 11:14:44,297 - __mp_main__ - INFO - Successfully loaded QA model
2024-12-26 11:14:45,367 - __mp_main__ - ERROR - Error loading summarization model: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>). See the original errors:

while loading with AutoModelForSeq2SeqLM, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\auto\auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

while loading with BartForConditionalGeneration, an error is thrown:
Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3897, in from_pretrained
    ).start()
      ^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\context.py", line 337, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\popen_spawn_win32.py", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\multiprocessing\spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\pipelines\base.py", line 289, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksjyo\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\modeling_utils.py", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'sshleifer/distilbart-cnn-12-6'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sshleifer/distilbart-cnn-12-6' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.



2024-12-26 11:14:52,044 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:52] "GET / HTTP/1.1" 200 -
2024-12-26 11:14:52,101 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:52] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-26 11:14:52,112 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:52] "[36mGET /static/js/main.js HTTP/1.1[0m" 304 -
2024-12-26 11:14:54,740 - __main__ - INFO - File saved successfully: a9c9d0100f3d912e2993936d86c34431e45d3a0aa548b582087f5f314f218426
2024-12-26 11:14:54,769 - __main__ - INFO - Temporary file removed: a9c9d0100f3d912e2993936d86c34431e45d3a0aa548b582087f5f314f218426
2024-12-26 11:14:54,770 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:54] "POST /upload HTTP/1.1" 200 -
2024-12-26 11:14:56,435 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:14:56,435 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:14:56,435 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:56] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:14:56,436 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:14:56] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:02,848 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:15:02,848 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:02] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:02,851 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:15:02,851 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:02] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:07,791 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:07] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:07,801 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:07] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:15,809 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:15:15,809 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:15] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:15,811 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:15:15,812 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:15] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:21,530 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:21] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:21,536 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:21] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:26,503 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:15:26,504 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:26] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:26,508 - __main__ - WARNING - Potential hallucination detected.
2024-12-26 11:15:26,508 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:26] "POST /ask HTTP/1.1" 200 -
2024-12-26 11:15:37,188 - werkzeug - INFO - 127.0.0.1 - - [26/Dec/2024 11:15:37] "POST /summarize HTTP/1.1" 200 -
